{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c50355",
   "metadata": {},
   "source": [
    "# HW5 Notebook\n",
    "Name: **Tony Nguyen**\n",
    "\n",
    "Class: CPSC 322 01\n",
    "\n",
    "Term: Fall 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084b3952",
   "metadata": {},
   "source": [
    "## 1. Load libraries and datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05155db4",
   "metadata": {},
   "source": [
    "### 1.1 Import the data table and utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed147760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_table import *\n",
    "from data_learn import *\n",
    "from data_eval import *\n",
    "from data_util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11de2c0",
   "metadata": {},
   "source": [
    "### 1.2 Load and clean auto data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0decd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  mpg    cyls    disp    hp    weight    accl    year    origin  name\n",
       "-----  ------  ------  ----  --------  ------  ------  --------  -------------------------\n",
       "   18       8     307   130      3504    12        70         1  chevrolet chevelle malibu\n",
       "   15       8     350   165      3693    11.5      70         1  buick skylark 320\n",
       "   18       8     318   150      3436    11        70         1  plymouth satellite\n",
       "   16       8     304   150      3433    12        70         1  amc rebel sst\n",
       "   17       8     302   140      3449    10.5      70         1  ford torino\n",
       "   15       8     429   198      4341    10        70         1  ford galaxie 500\n",
       "   14       8     454   220      4354     9        70         1  chevrolet impala\n",
       "   14       8     440   215      4312     8.5      70         1  plymouth fury iii\n",
       "   14       8     455   225      4425    10        70         1  pontiac catalina\n",
       "   15       8     390   190      3850     8.5      70         1  amc ambassador dpl"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto = DataTable(['mpg','cyls','disp','hp','weight','accl','year','origin','name'])\n",
    "auto.load('auto-mpg.txt')\n",
    "auto.rows(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94f256ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before cleaning: 317\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows before cleaning:\", auto.row_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966c51cf",
   "metadata": {},
   "source": [
    "### 1.3 Remove duplicates rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1786b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after removing duplicated rows: 316\n"
     ]
    }
   ],
   "source": [
    "auto = remove_duplicates(auto)\n",
    "print(\"Number of rows after removing duplicated rows:\", auto.row_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4f3ae2",
   "metadata": {},
   "source": [
    "### 1.3 Remove missing rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20383f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after removing missing values: 307\n"
     ]
    }
   ],
   "source": [
    "auto = remove_missing(auto, auto.columns())\n",
    "print(\"Number of rows after removing missing values:\", auto.row_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b5c32c",
   "metadata": {},
   "source": [
    "## 2. Exploring k-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eafb16f",
   "metadata": {},
   "source": [
    "### 2.1 Discretize MPG values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b87f8b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_copy = auto.copy()\n",
    "mpg_value = auto_copy.get_column_data('mpg')\n",
    "mpg_value = sorted(mpg_value)\n",
    "bin_width = (mpg_value[-1] - mpg_value[0]) / 3\n",
    "cut_points = [mpg_value[0] + bin_width, mpg_value[0] + 2 * bin_width]\n",
    "discretize(auto_copy, 'mpg', cut_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ed6d92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  mpg    cyls    disp    hp    weight    accl    year    origin  name\n",
       "-----  ------  ------  ----  --------  ------  ------  --------  ----------------------------\n",
       "    1       8     307   130      3504    12        70         1  chevrolet chevelle malibu\n",
       "    1       8     350   165      3693    11.5      70         1  buick skylark 320\n",
       "    1       8     318   150      3436    11        70         1  plymouth satellite\n",
       "    1       8     304   150      3433    12        70         1  amc rebel sst\n",
       "    1       8     302   140      3449    10.5      70         1  ford torino\n",
       "    1       8     429   198      4341    10        70         1  ford galaxie 500\n",
       "    1       8     454   220      4354     9        70         1  chevrolet impala\n",
       "    1       8     440   215      4312     8.5      70         1  plymouth fury iii\n",
       "    1       8     455   225      4425    10        70         1  pontiac catalina\n",
       "    1       8     390   190      3850     8.5      70         1  amc ambassador dpl\n",
       "    1       8     383   170      3563    10        70         1  dodge challenger se\n",
       "    1       8     340   160      3609     8        70         1  plymouth 'cuda 340\n",
       "    1       8     400   150      3761     9.5      70         1  chevrolet monte carlo s\n",
       "    1       8     455   225      3086    10        70         1  buick estate wagon (sw)\n",
       "    2       4     113    95      2372    15        70         3  toyota corona mark ii\n",
       "    2       6     198    95      2833    15.5      70         1  plymouth duster\n",
       "    1       6     199    97      2774    15.5      70         1  amc hornet\n",
       "    2       6     200    85      2587    16        70         1  ford maverick\n",
       "    2       4      97    88      2130    14.5      70         3  datsun pl510\n",
       "    2       4      97    46      1835    20.5      70         2  volkswagen 1131 deluxe sedan"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_copy.rows(range(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c17f471",
   "metadata": {},
   "source": [
    "### 2.2 Normalize all columns except model and origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "806f56bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize(auto_copy, 'cyls')\n",
    "normalize(auto_copy, 'disp')\n",
    "normalize(auto_copy, 'hp')\n",
    "normalize(auto_copy, 'weight')\n",
    "normalize(auto_copy, 'accl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6faccf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  mpg    cyls       disp        hp    weight       accl    year    origin  name\n",
       "-----  ------  ---------  --------  --------  ---------  ------  --------  ----------------------------\n",
       "    1     1    0.617571   0.456522  0.53615   0.238095       70         1  chevrolet chevelle malibu\n",
       "    1     1    0.728682   0.646739  0.589736  0.208333       70         1  buick skylark 320\n",
       "    1     1    0.645995   0.565217  0.51687   0.178571       70         1  plymouth satellite\n",
       "    1     1    0.609819   0.565217  0.516019  0.238095       70         1  amc rebel sst\n",
       "    1     1    0.604651   0.51087   0.520556  0.14881        70         1  ford torino\n",
       "    1     1    0.932817   0.826087  0.773462  0.119048       70         1  ford galaxie 500\n",
       "    1     1    0.997416   0.945652  0.777148  0.0595238      70         1  chevrolet impala\n",
       "    1     1    0.96124    0.918478  0.76524   0.0297619      70         1  plymouth fury iii\n",
       "    1     1    1          0.972826  0.797278  0.119048       70         1  pontiac catalina\n",
       "    1     1    0.832041   0.782609  0.63425   0.0297619      70         1  amc ambassador dpl\n",
       "    1     1    0.813953   0.673913  0.552878  0.119048       70         1  dodge challenger se\n",
       "    1     1    0.702842   0.619565  0.56592   0              70         1  plymouth 'cuda 340\n",
       "    1     1    0.857881   0.565217  0.609016  0.0892857      70         1  chevrolet monte carlo s\n",
       "    1     1    1          0.972826  0.417635  0.119048       70         1  buick estate wagon (sw)\n",
       "    2     0.2  0.116279   0.266304  0.215197  0.416667       70         3  toyota corona mark ii\n",
       "    2     0.6  0.335917   0.266304  0.345903  0.446429       70         1  plymouth duster\n",
       "    1     0.6  0.338501   0.277174  0.329175  0.446429       70         1  amc hornet\n",
       "    2     0.6  0.341085   0.211957  0.276155  0.47619        70         1  ford maverick\n",
       "    2     0.2  0.0749354  0.228261  0.146583  0.386905       70         3  datsun pl510\n",
       "    2     0.2  0.0749354  0         0.062943  0.744048       70         2  volkswagen 1131 deluxe sedan"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_copy.rows(range(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfa9df7",
   "metadata": {},
   "source": [
    "### 2.3 Create train and test sets using Holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0998131",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = holdout(auto_copy, auto_copy.row_count()//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfea6ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  mpg    cyls      disp        hp    weight       accl    year    origin  name\n",
       "-----  ------  --------  --------  --------  ---------  ------  --------  -----------------------\n",
       "    1       1  0.645995  0.565217  0.51687   0.178571       70         1  plymouth satellite\n",
       "    1       1  0.604651  0.51087   0.520556  0.14881        70         1  ford torino\n",
       "    1       1  0.932817  0.826087  0.773462  0.119048       70         1  ford galaxie 500\n",
       "    1       1  0.997416  0.945652  0.777148  0.0595238      70         1  chevrolet impala\n",
       "    1       1  1         0.972826  0.797278  0.119048       70         1  pontiac catalina\n",
       "    1       1  0.832041  0.782609  0.63425   0.0297619      70         1  amc ambassador dpl\n",
       "    1       1  0.813953  0.673913  0.552878  0.119048       70         1  dodge challenger se\n",
       "    1       1  0.702842  0.619565  0.56592   0              70         1  plymouth 'cuda 340\n",
       "    1       1  0.857881  0.565217  0.609016  0.0892857      70         1  chevrolet monte carlo s\n",
       "    1       1  1         0.972826  0.417635  0.119048       70         1  buick estate wagon (sw)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.rows(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04f64064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  mpg    cyls        disp        hp     weight      accl    year    origin  name\n",
       "-----  ------  ----------  --------  ---------  --------  ------  --------  -----------------------\n",
       "    3     0.2  0.0594315   0.076087  0.0530196  0.5           78         3  honda civic cvcc\n",
       "    1     1    0.857881    0.782609  0.768925   0.25          77         1  chrysler cordoba\n",
       "    1     1    0.728682    0.592391  0.778849   0.410714      79         1  buick estate wagon (sw)\n",
       "    3     0.2  0.00775194  0.103261  0.0632265  0.77381       74         3  toyota corolla 1200\n",
       "    2     0.2  0.186047    0.141304  0.223419   0.684524      73         1  chevrolet vega\n",
       "    1     0.6  0.470284    0.173913  0.555997   0.77381       76         1  ford granada ghia\n",
       "    1     0.6  0.338501    0.277174  0.329175   0.446429      70         1  amc hornet\n",
       "    2     0.2  0.136951    0.375     0.299972   0.327381      75         2  saab 99le\n",
       "    2     0.2  0.22739     0.320652  0.320953   0.517857      78         1  plymouth sapporo\n",
       "    1     0.6  0.470284    0.141304  0.438049   0.684524      75         1  ford maverick"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.rows(range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7c86f1",
   "metadata": {},
   "source": [
    "### 2.4 Run k-NN over the train and test set\n",
    "* k = 5\n",
    "* Majority vote\n",
    "* Numerical columns: cylinders, weight, and acceleration\n",
    "* No nominal attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cc4dc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = knn_eval(train, test, majority_vote, 5, 'mpg', ['cyls','weight','accl'], [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7406523d",
   "metadata": {},
   "source": [
    "### 2.5 Print the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bf17366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual      1    2    3    total\n",
      "--------  ---  ---  ---  -------\n",
      "1          60   15    2       77\n",
      "2           6   44   13       63\n",
      "3           0    4    9       13\n",
      "total      66   63   24      153\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7de7d4",
   "metadata": {},
   "source": [
    "### 2.6 Calculate the Accuracy & Macro average f-measure for MPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fdd6a2",
   "metadata": {},
   "source": [
    "Find the number of non-empty classes (L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f619568f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-empty labels count: 3\n"
     ]
    }
   ],
   "source": [
    "sum_of_actual_labels = result.get_column_data('total')\n",
    "non_empty_label = 0\n",
    "for i in sum_of_actual_labels:\n",
    "    if i != 0:\n",
    "        non_empty_label += 1\n",
    "\n",
    "if result[result.row_count() - 1]['actual'] == 'total':\n",
    "    non_empty_label -= 1\n",
    "\n",
    "print(\"Non-empty labels count:\", non_empty_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80223b0b",
   "metadata": {},
   "source": [
    "Find the average accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27d2c191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.8257080610021786\n"
     ]
    }
   ],
   "source": [
    "total_accuracy = 0\n",
    "for i in range(1, 4): # labels are 1, 2, 3\n",
    "    num = accuracy(result, i)\n",
    "    if num != -1:\n",
    "        total_accuracy += num\n",
    "\n",
    "print(\"Average accuracy:\", total_accuracy / non_empty_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5c103f",
   "metadata": {},
   "source": [
    "Find macro-average f-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ce6c67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6608345358345358\n"
     ]
    }
   ],
   "source": [
    "total_precision = 0\n",
    "denum = non_empty_label\n",
    "for i in range(1, 4): # labels are 1, 2, 3\n",
    "    num = precision(result, i)\n",
    "    if num != -1:\n",
    "        total_precision += num\n",
    "    else:\n",
    "        print(\"p_prediced is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "        denum -= 1\n",
    "\n",
    "precision_m = total_precision / denum\n",
    "print(\"Precision:\", precision_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "645c9bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.7233137233137233\n"
     ]
    }
   ],
   "source": [
    "total_recall = 0\n",
    "denum = non_empty_label\n",
    "for i in range(1, 4): # labels are 1, 2, 3\n",
    "    num = recall(result, i)\n",
    "    if num != -1:\n",
    "        total_recall += num\n",
    "    else:\n",
    "        print(\"p_actual is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "        denum -= 1\n",
    "\n",
    "recall_m = total_recall / denum\n",
    "print(\"Recall:\", recall_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0aed2a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-score: 0.6906640028618143\n"
     ]
    }
   ],
   "source": [
    "f_score = 2 * precision_m * recall_m / (precision_m + recall_m)\n",
    "print(\"F-score:\", f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480eb0f3",
   "metadata": {},
   "source": [
    "### 2.7 Average result after running three times using Majority Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c8d93c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "--------------------\n",
      "actual      1    2    3    total\n",
      "--------  ---  ---  ---  -------\n",
      "1          65   11    1       77\n",
      "2           6   58    1       65\n",
      "3           0   11    0       11\n",
      "total      71   80    2      153\n",
      "\n",
      "Non-empty labels count: 3\n",
      "Average accuracy: 0.869281045751634\n",
      "Precision: 0.546830985915493\n",
      "Recall: 0.5788211788211788\n",
      "Macro-averaging F-score: 0.5623715136861966\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2\n",
      "--------------------\n",
      "actual      1    2    3    total\n",
      "--------  ---  ---  ---  -------\n",
      "1          68    5    1       74\n",
      "2          14   35   17       66\n",
      "3           0    4    9       13\n",
      "total      82   44   27      153\n",
      "\n",
      "Non-empty labels count: 3\n",
      "Average accuracy: 0.8213507625272332\n",
      "Precision: 0.6526853904902684\n",
      "Recall: 0.7138432138432137\n",
      "Macro-averaging F-score: 0.6818957690290486\n",
      "\n",
      "Iteration 3\n",
      "--------------------\n",
      "actual      1    2    3    total\n",
      "--------  ---  ---  ---  -------\n",
      "1          72    8    0       80\n",
      "2           8   54    0       62\n",
      "3           0   11    0       11\n",
      "total      80   73    0      153\n",
      "\n",
      "Non-empty labels count: 3\n",
      "Average accuracy: 0.8823529411764707\n",
      "p_prediced is 0 for label 3 . Subtracted 1 from non_empty_label.\n",
      "Precision: 0.5465753424657535\n",
      "Recall: 0.885483870967742\n",
      "Macro-averaging F-score: 0.6759268687803749\n",
      "\n",
      "Summary\n",
      "--------------------\n",
      "Average accuracy after 3 iterations: 0.8576615831517792\n",
      "Average precision after 3 iterations: 0.5820305729571715\n",
      "Average recall after 3 iterations: 0.7260494212107114\n",
      "Average F-score after 3 iterations: 0.6400647171652066\n"
     ]
    }
   ],
   "source": [
    "total_accuracy_after_iteration = 0\n",
    "total_precision_after_iteration = 0\n",
    "total_recall_after_iteration = 0\n",
    "total_f_score_after_iteration = 0\n",
    "\n",
    "for i in range(3):\n",
    "    # Discretize\n",
    "    auto_copy = auto.copy()\n",
    "    mpg_value = auto_copy.get_column_data('mpg')\n",
    "    mpg_value = sorted(mpg_value)\n",
    "    bin_width = (mpg_value[-1] - mpg_value[0]) / 3\n",
    "    cut_points = [mpg_value[0] + bin_width, mpg_value[0] + 2 * bin_width]\n",
    "    discretize(auto_copy, 'mpg', cut_points)\n",
    "\n",
    "    # Normalize\n",
    "    normalize(auto_copy, 'cyls')\n",
    "    normalize(auto_copy, 'disp')\n",
    "    normalize(auto_copy, 'hp')\n",
    "    normalize(auto_copy, 'weight')\n",
    "    normalize(auto_copy, 'accl')\n",
    "\n",
    "    # Holdout\n",
    "    train, test = holdout(auto_copy, auto_copy.row_count()//2)\n",
    "\n",
    "    # kNN\n",
    "    result = knn_eval(train, test, majority_vote, 5, 'mpg', ['cyls','weight','accl'], [])\n",
    "    label_list = result.get_column_data('actual')\n",
    "    if 'total' in label_list:\n",
    "        label_list.remove('total')\n",
    "\n",
    "    print(\"Iteration\", i+1)\n",
    "    print(\"--------------------\")\n",
    "    print(result)\n",
    "    print()\n",
    "\n",
    "    # Find non-empty labels\n",
    "    sum_of_actual_labels = result.get_column_data('total')\n",
    "    non_empty_label = 0\n",
    "    for i in sum_of_actual_labels:\n",
    "        if i != 0:\n",
    "            non_empty_label += 1\n",
    "\n",
    "    if result[result.row_count() - 1]['actual'] == 'total':\n",
    "        non_empty_label -= 1\n",
    "\n",
    "    print(\"Non-empty labels count:\", non_empty_label)\n",
    "\n",
    "    # Accuracy\n",
    "    total_accuracy = 0\n",
    "    for i in label_list: \n",
    "        num = accuracy(result, i)\n",
    "        if num != -1:\n",
    "            total_accuracy += num\n",
    "    acc = total_accuracy / non_empty_label\n",
    "    total_accuracy_after_iteration += acc\n",
    "    print(\"Average accuracy:\", acc)\n",
    "\n",
    "    # Precision\n",
    "    total_precision = 0\n",
    "    denum = non_empty_label\n",
    "    for i in label_list:\n",
    "        num = precision(result, i)\n",
    "        if num != -1:\n",
    "            total_precision += num\n",
    "        else:\n",
    "            print(\"p_prediced is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "            non_empty_label -= 1\n",
    "    precision_m = total_precision / denum\n",
    "    total_precision_after_iteration += precision_m\n",
    "    print(\"Precision:\", precision_m)\n",
    "\n",
    "    # Recall\n",
    "    total_recall = 0\n",
    "    denum = non_empty_label\n",
    "    for i in label_list: # labels are 1, 2, 3\n",
    "        num = recall(result, i)\n",
    "        if num != -1:\n",
    "            total_recall += num\n",
    "        else:\n",
    "            print(\"p_actual is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "            denum -= 1\n",
    "    recall_m = total_recall / non_empty_label\n",
    "    total_recall_after_iteration += recall_m\n",
    "    print(\"Recall:\", recall_m)\n",
    "\n",
    "    # F-score\n",
    "    f_score = 2 * precision_m * recall_m / (precision_m + recall_m)\n",
    "    total_f_score_after_iteration += f_score\n",
    "    print(\"Macro-averaging F-score:\", f_score)\n",
    "    print()\n",
    "\n",
    "print(\"Summary\")\n",
    "print(\"--------------------\")\n",
    "print(\"Average accuracy after 3 iterations:\", total_accuracy_after_iteration / 3)\n",
    "print(\"Average precision after 3 iterations:\", total_precision_after_iteration / 3)\n",
    "print(\"Average recall after 3 iterations:\", total_recall_after_iteration / 3)\n",
    "print(\"Average F-score after 3 iterations:\", total_f_score_after_iteration / 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba01062b",
   "metadata": {},
   "source": [
    "### 2.8 Average result after running three times using Weighted Vote\n",
    "Run the kNN algorithm again\n",
    "* k = 5\n",
    "* Weighted vote\n",
    "* Numerical columns: cylinders, weight, and acceleration\n",
    "* No nominal attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c233dc8",
   "metadata": {},
   "source": [
    "Find the accuracy and macro-average f-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c5208f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "--------------------\n",
      "actual      1    2    3    total\n",
      "--------  ---  ---  ---  -------\n",
      "1          48   28    7       83\n",
      "2           6    6   48       60\n",
      "3           0    0   10       10\n",
      "total      54   34   65      153\n",
      "\n",
      "Non-empty labels count: 3\n",
      "Average accuracy: 0.6122004357298475\n",
      "Precision: 0.4064018769901123\n",
      "Recall: 0.559437751004016\n",
      "Macro-averaging F-score: 0.47079565898396\n",
      "\n",
      "Iteration 2\n",
      "--------------------\n",
      "actual      1    2    3    total\n",
      "--------  ---  ---  ---  -------\n",
      "1          42   29    4       75\n",
      "2           3   32   32       67\n",
      "3           0    1   10       11\n",
      "total      45   62   46      153\n",
      "\n",
      "Non-empty labels count: 3\n",
      "Average accuracy: 0.699346405228758\n",
      "Precision: 0.5556178899797414\n",
      "Recall: 0.6489009497964723\n",
      "Macro-averaging F-score: 0.5986473015212458\n",
      "\n",
      "Iteration 3\n",
      "--------------------\n",
      "actual      1    2    3    total\n",
      "--------  ---  ---  ---  -------\n",
      "1          16   59    5       80\n",
      "2           5   12   47       64\n",
      "3           0    0    9        9\n",
      "total      21   71   61      153\n",
      "\n",
      "Non-empty labels count: 3\n",
      "Average accuracy: 0.4945533769063181\n",
      "Precision: 0.3594866100061205\n",
      "Recall: 0.46249999999999997\n",
      "Macro-averaging F-score: 0.40453835890731293\n",
      "\n",
      "Summary\n",
      "--------------------\n",
      "Average accuracy after 3 iterations: 0.6020334059549746\n",
      "Average precision after 3 iterations: 0.4405021256586581\n",
      "Average recall after 3 iterations: 0.5569462336001627\n",
      "Average F-score after 3 iterations: 0.4913271064708396\n"
     ]
    }
   ],
   "source": [
    "total_accuracy_after_iteration = 0\n",
    "total_precision_after_iteration = 0\n",
    "total_recall_after_iteration = 0\n",
    "total_f_score_after_iteration = 0\n",
    "\n",
    "for i in range(3):\n",
    "    # Discretize\n",
    "    auto_copy = auto.copy()\n",
    "    mpg_value = auto_copy.get_column_data('mpg')\n",
    "    mpg_value = sorted(mpg_value)\n",
    "    bin_width = (mpg_value[-1] - mpg_value[0]) / 3\n",
    "    cut_points = [mpg_value[0] + bin_width, mpg_value[0] + 2 * bin_width]\n",
    "    discretize(auto_copy, 'mpg', cut_points)\n",
    "\n",
    "    # Normalize\n",
    "    normalize(auto_copy, 'cyls')\n",
    "    normalize(auto_copy, 'disp')\n",
    "    normalize(auto_copy, 'hp')\n",
    "    normalize(auto_copy, 'weight')\n",
    "    normalize(auto_copy, 'accl')\n",
    "\n",
    "    # Holdout\n",
    "    train, test = holdout(auto_copy, auto_copy.row_count()//2)\n",
    "\n",
    "    # kNN\n",
    "    result = knn_eval(train, test, weighted_vote, 5, 'mpg', ['cyls','weight','accl'], [])\n",
    "    label_list = result.get_column_data('actual')\n",
    "    if 'total' in label_list:\n",
    "        label_list.remove('total')\n",
    "\n",
    "    print(\"Iteration\", i+1)\n",
    "    print(\"--------------------\")\n",
    "    print(result)\n",
    "    print()\n",
    "\n",
    "    # Find non-empty labels\n",
    "    sum_of_actual_labels = result.get_column_data('total')\n",
    "    non_empty_label = 0\n",
    "    for i in sum_of_actual_labels:\n",
    "        if i != 0:\n",
    "            non_empty_label += 1\n",
    "\n",
    "    if result[result.row_count() - 1]['actual'] == 'total':\n",
    "        non_empty_label -= 1\n",
    "\n",
    "    print(\"Non-empty labels count:\", non_empty_label)\n",
    "\n",
    "    # Accuracy\n",
    "    total_accuracy = 0\n",
    "    for i in label_list: \n",
    "        num = accuracy(result, i)\n",
    "        if num != -1:\n",
    "            total_accuracy += num\n",
    "    acc = total_accuracy / non_empty_label\n",
    "    total_accuracy_after_iteration += acc\n",
    "    print(\"Average accuracy:\", acc)\n",
    "\n",
    "    # Precision\n",
    "    total_precision = 0\n",
    "    denum = non_empty_label\n",
    "    for i in label_list:\n",
    "        num = precision(result, i)\n",
    "        if num != -1:\n",
    "            total_precision += num\n",
    "        else:\n",
    "            print(\"p_prediced is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "            non_empty_label -= 1\n",
    "    precision_m = total_precision / denum\n",
    "    total_precision_after_iteration += precision_m\n",
    "    print(\"Precision:\", precision_m)\n",
    "\n",
    "    # Recall\n",
    "    total_recall = 0\n",
    "    denum = non_empty_label\n",
    "    for i in label_list: # labels are 1, 2, 3\n",
    "        num = recall(result, i)\n",
    "        if num != -1:\n",
    "            total_recall += num\n",
    "        else:\n",
    "            print(\"p_actual is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "            denum -= 1\n",
    "    recall_m = total_recall / non_empty_label\n",
    "    total_recall_after_iteration += recall_m\n",
    "    print(\"Recall:\", recall_m)\n",
    "\n",
    "    # F-score\n",
    "    f_score = 2 * precision_m * recall_m / (precision_m + recall_m)\n",
    "    total_f_score_after_iteration += f_score\n",
    "    print(\"Macro-averaging F-score:\", f_score)\n",
    "    print()\n",
    "\n",
    "print(\"Summary\")\n",
    "print(\"--------------------\")\n",
    "print(\"Average accuracy after 3 iterations:\", total_accuracy_after_iteration / 3)\n",
    "print(\"Average precision after 3 iterations:\", total_precision_after_iteration / 3)\n",
    "print(\"Average recall after 3 iterations:\", total_recall_after_iteration / 3)\n",
    "print(\"Average F-score after 3 iterations:\", total_f_score_after_iteration / 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356f1246",
   "metadata": {},
   "source": [
    "### 2.9 Compare 2.6 and 2.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4140ff1",
   "metadata": {},
   "source": [
    "Between the two, the results are as follow:\n",
    "\n",
    "|Metrics|2.6|2.7|\n",
    "|---|---|---|\n",
    "|Accuracy|0.8257|0.8577|\n",
    "|Precision|0.6608|0.5820|\n",
    "|Recall|0.7233|0.7260|\n",
    "|F-score|0.6907|0.6401|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc73dab",
   "metadata": {},
   "source": [
    "* The overall accuracy is higher in 2.7 than 2.6, although they are not siginificantly different.\n",
    "* The precision is higher in 2.6 than 2.7. This is interesting as the gap is quite big. This might be cause by the Holdout method as it does not maintain the distribution.\n",
    "* The recall is higher in 2.7 than 2.6, though very slightly.\n",
    "* The F-score is higher in 2.6 than 2.7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7946761",
   "metadata": {},
   "source": [
    "### 2.10 Redo 2.6 and compare the results at two different bin sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8302f34a",
   "metadata": {},
   "source": [
    "At bin=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00d5d551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "--------------------\n",
      "actual      1    2    3    4    total\n",
      "--------  ---  ---  ---  ---  -------\n",
      "1          25   24    1    0       50\n",
      "2           0   63    8    0       71\n",
      "3           0   26    1    0       27\n",
      "4           0    5    0    0        5\n",
      "total      25  118   10    0      153\n",
      "\n",
      "Non-empty labels count: 4\n",
      "Average accuracy: 0.7908496732026145\n",
      "p_prediced is 0 for label 4 . Subtracted 1 from non_empty_label.\n",
      "Precision: 0.40847457627118644\n",
      "Recall: 0.4747869935663363\n",
      "Macro-averaging F-score: 0.4391415241846308\n",
      "\n",
      "Iteration 2\n",
      "--------------------\n",
      "actual      1    2    3    4    total\n",
      "--------  ---  ---  ---  ---  -------\n",
      "1          46    8    0    0       54\n",
      "2           7   44   15    0       66\n",
      "3           0   24    7    0       31\n",
      "4           0    2    0    0        2\n",
      "total      53   78   22    0      153\n",
      "\n",
      "Non-empty labels count: 4\n",
      "Average accuracy: 0.8169934640522876\n",
      "p_prediced is 0 for label 4 . Subtracted 1 from non_empty_label.\n",
      "Precision: 0.43755222764656726\n",
      "Recall: 0.581441656710474\n",
      "Macro-averaging F-score: 0.4993378194820185\n",
      "\n",
      "Iteration 3\n",
      "--------------------\n",
      "actual      1    2    3    4    total\n",
      "--------  ---  ---  ---  ---  -------\n",
      "1          39    7    0    0       46\n",
      "2           4   43   24    0       71\n",
      "3           0    6   24    0       30\n",
      "4           0    0    6    0        6\n",
      "total      43   56   54    0      153\n",
      "\n",
      "Non-empty labels count: 4\n",
      "Average accuracy: 0.8464052287581699\n",
      "p_prediced is 0 for label 4 . Subtracted 1 from non_empty_label.\n",
      "Precision: 0.5298195828719084\n",
      "Recall: 0.751153296591141\n",
      "Macro-averaging F-score: 0.6213647964812469\n",
      "\n",
      "Summary\n",
      "--------------------\n",
      "Average accuracy after 3 iterations: 0.818082788671024\n",
      "Average precision after 3 iterations: 0.45861546226322075\n",
      "Average recall after 3 iterations: 0.6024606489559838\n",
      "Average F-score after 3 iterations: 0.5199480467159654\n"
     ]
    }
   ],
   "source": [
    "total_accuracy_after_iteration = 0\n",
    "total_precision_after_iteration = 0\n",
    "total_recall_after_iteration = 0\n",
    "total_f_score_after_iteration = 0\n",
    "\n",
    "for i in range(3):\n",
    "    # Discretize\n",
    "    auto_copy = auto.copy()\n",
    "    mpg_value = auto_copy.get_column_data('mpg')\n",
    "    mpg_value = sorted(mpg_value)\n",
    "    bin_width = (mpg_value[-1] - mpg_value[0]) / 4\n",
    "    cut_points = [mpg_value[0] + bin_width, mpg_value[0] + 2 * bin_width, mpg_value[0] + 3 * bin_width]\n",
    "    discretize(auto_copy, 'mpg', cut_points)\n",
    "\n",
    "    # Normalize\n",
    "    normalize(auto_copy, 'cyls')\n",
    "    normalize(auto_copy, 'disp')\n",
    "    normalize(auto_copy, 'hp')\n",
    "    normalize(auto_copy, 'weight')\n",
    "    normalize(auto_copy, 'accl')\n",
    "\n",
    "    # Holdout\n",
    "    train, test = holdout(auto_copy, auto_copy.row_count()//2)\n",
    "\n",
    "    # kNN\n",
    "    result = knn_eval(train, test, majority_vote, 5, 'mpg', ['cyls','weight','accl'], [])\n",
    "    label_list = result.get_column_data('actual')\n",
    "    if 'total' in label_list:\n",
    "        label_list.remove('total')\n",
    "\n",
    "    print(\"Iteration\", i+1)\n",
    "    print(\"--------------------\")\n",
    "    print(result)\n",
    "    print()\n",
    "\n",
    "    # Find non-empty labels\n",
    "    sum_of_actual_labels = result.get_column_data('total')\n",
    "    non_empty_label = 0\n",
    "    for i in sum_of_actual_labels:\n",
    "        if i != 0:\n",
    "            non_empty_label += 1\n",
    "\n",
    "    if result[result.row_count() - 1]['actual'] == 'total':\n",
    "        non_empty_label -= 1\n",
    "\n",
    "    print(\"Non-empty labels count:\", non_empty_label)\n",
    "\n",
    "    # Accuracy\n",
    "    total_accuracy = 0\n",
    "    for i in label_list: \n",
    "        num = accuracy(result, i)\n",
    "        if num != -1:\n",
    "            total_accuracy += num\n",
    "    acc = total_accuracy / non_empty_label\n",
    "    total_accuracy_after_iteration += acc\n",
    "    print(\"Average accuracy:\", acc)\n",
    "\n",
    "    # Precision\n",
    "    total_precision = 0\n",
    "    denum = non_empty_label\n",
    "    for i in label_list:\n",
    "        num = precision(result, i)\n",
    "        if num != -1:\n",
    "            total_precision += num\n",
    "        else:\n",
    "            print(\"p_prediced is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "            non_empty_label -= 1\n",
    "    precision_m = total_precision / denum\n",
    "    total_precision_after_iteration += precision_m\n",
    "    print(\"Precision:\", precision_m)\n",
    "\n",
    "    # Recall\n",
    "    total_recall = 0\n",
    "    denum = non_empty_label\n",
    "    for i in label_list: # labels are 1, 2, 3\n",
    "        num = recall(result, i)\n",
    "        if num != -1:\n",
    "            total_recall += num\n",
    "        else:\n",
    "            print(\"p_actual is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "            denum -= 1\n",
    "    recall_m = total_recall / non_empty_label\n",
    "    total_recall_after_iteration += recall_m\n",
    "    print(\"Recall:\", recall_m)\n",
    "\n",
    "    # F-score\n",
    "    f_score = 2 * precision_m * recall_m / (precision_m + recall_m)\n",
    "    total_f_score_after_iteration += f_score\n",
    "    print(\"Macro-averaging F-score:\", f_score)\n",
    "    print()\n",
    "\n",
    "print(\"Summary\")\n",
    "print(\"--------------------\")\n",
    "print(\"Average accuracy after 3 iterations:\", total_accuracy_after_iteration / 3)\n",
    "print(\"Average precision after 3 iterations:\", total_precision_after_iteration / 3)\n",
    "print(\"Average recall after 3 iterations:\", total_recall_after_iteration / 3)\n",
    "print(\"Average F-score after 3 iterations:\", total_f_score_after_iteration / 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659dc8c1",
   "metadata": {},
   "source": [
    "At bin=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfc99939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "--------------------\n",
      "actual      1    2    3    4    5    total\n",
      "--------  ---  ---  ---  ---  ---  -------\n",
      "1          22   11    0    0    0       33\n",
      "2           2   44   10    0    0       56\n",
      "3           1    1   41    0    0       43\n",
      "4           0    0   20    0    0       20\n",
      "5           0    0    1    0    0        1\n",
      "total      25   56   72    0    0      153\n",
      "\n",
      "Non-empty labels count: 5\n",
      "Average accuracy: 0.8797385620915034\n",
      "p_prediced is 0 for label 4 . Subtracted 1 from non_empty_label.\n",
      "p_prediced is 0 for label 5 . Subtracted 1 from non_empty_label.\n",
      "Precision: 0.447031746031746\n",
      "Recall: 0.8019564414913253\n",
      "Macro-averaging F-score: 0.5740646578767594\n",
      "\n",
      "Iteration 2\n",
      "--------------------\n",
      "actual      1    2    3    4    5    total\n",
      "--------  ---  ---  ---  ---  ---  -------\n",
      "1          25   14    0    0    0       39\n",
      "2          13   33   11    0    0       57\n",
      "3           1    3   33    2    0       39\n",
      "4           0    0   16    0    0       16\n",
      "5           0    0    2    0    0        2\n",
      "total      39   50   62    2    0      153\n",
      "\n",
      "Non-empty labels count: 5\n",
      "Average accuracy: 0.837908496732026\n",
      "p_prediced is 0 for label 5 . Subtracted 1 from non_empty_label.\n",
      "Precision: 0.366656741108354\n",
      "Recall: 0.516531713900135\n",
      "Macro-averaging F-score: 0.4288775149261112\n",
      "\n",
      "Iteration 3\n",
      "--------------------\n",
      "actual      1    2    3    4    5    total\n",
      "--------  ---  ---  ---  ---  ---  -------\n",
      "1          20   10    0    0    0       30\n",
      "2           6   43    5    6    0       60\n",
      "3           0    5    6   33    0       44\n",
      "4           0    0    1   17    0       18\n",
      "5           0    0    0    1    0        1\n",
      "total      26   58   12   57    0      153\n",
      "\n",
      "Non-empty labels count: 5\n",
      "Average accuracy: 0.8248366013071895\n",
      "p_prediced is 0 for label 5 . Subtracted 1 from non_empty_label.\n",
      "Precision: 0.46177113872213693\n",
      "Recall: 0.6160353535353535\n",
      "Macro-averaging F-score: 0.5278634870704685\n",
      "\n",
      "Summary\n",
      "--------------------\n",
      "Average accuracy after 3 iterations: 0.8474945533769063\n",
      "Average precision after 3 iterations: 0.4251532086207456\n",
      "Average recall after 3 iterations: 0.6448411696422712\n",
      "Average F-score after 3 iterations: 0.510268553291113\n"
     ]
    }
   ],
   "source": [
    "total_accuracy_after_iteration = 0\n",
    "total_precision_after_iteration = 0\n",
    "total_recall_after_iteration = 0\n",
    "total_f_score_after_iteration = 0\n",
    "\n",
    "for i in range(3):\n",
    "    # Discretize\n",
    "    auto_copy = auto.copy()\n",
    "    mpg_value = auto_copy.get_column_data('mpg')\n",
    "    mpg_value = sorted(mpg_value)\n",
    "    bin_width = (mpg_value[-1] - mpg_value[0]) / 5\n",
    "    cut_points = [mpg_value[0] + bin_width, mpg_value[0] + 2 * bin_width, mpg_value[0] + 3 * bin_width, mpg_value[0] + 4 * bin_width]\n",
    "    discretize(auto_copy, 'mpg', cut_points)\n",
    "\n",
    "    # Normalize\n",
    "    normalize(auto_copy, 'cyls')\n",
    "    normalize(auto_copy, 'disp')\n",
    "    normalize(auto_copy, 'hp')\n",
    "    normalize(auto_copy, 'weight')\n",
    "    normalize(auto_copy, 'accl')\n",
    "\n",
    "    # Holdout\n",
    "    train, test = holdout(auto_copy, auto_copy.row_count()//2)\n",
    "\n",
    "    # kNN\n",
    "    result = knn_eval(train, test, majority_vote, 5, 'mpg', ['cyls','weight','accl'], [])\n",
    "    label_list = result.get_column_data('actual')\n",
    "    if 'total' in label_list:\n",
    "        label_list.remove('total')\n",
    "\n",
    "    print(\"Iteration\", i+1)\n",
    "    print(\"--------------------\")\n",
    "    print(result)\n",
    "    print()\n",
    "\n",
    "    # Find non-empty labels\n",
    "    sum_of_actual_labels = result.get_column_data('total')\n",
    "    non_empty_label = 0\n",
    "    for i in sum_of_actual_labels:\n",
    "        if i != 0:\n",
    "            non_empty_label += 1\n",
    "\n",
    "    if result[result.row_count() - 1]['actual'] == 'total':\n",
    "        non_empty_label -= 1\n",
    "\n",
    "    print(\"Non-empty labels count:\", non_empty_label)\n",
    "\n",
    "    # Accuracy\n",
    "    total_accuracy = 0\n",
    "    for i in label_list: \n",
    "        num = accuracy(result, i)\n",
    "        if num != -1:\n",
    "            total_accuracy += num\n",
    "    acc = total_accuracy / non_empty_label\n",
    "    total_accuracy_after_iteration += acc\n",
    "    print(\"Average accuracy:\", acc)\n",
    "\n",
    "    # Precision\n",
    "    total_precision = 0\n",
    "    denum = non_empty_label\n",
    "    for i in label_list:\n",
    "        num = precision(result, i)\n",
    "        if num != -1:\n",
    "            total_precision += num\n",
    "        else:\n",
    "            print(\"p_prediced is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "            non_empty_label -= 1\n",
    "    precision_m = total_precision / denum\n",
    "    total_precision_after_iteration += precision_m\n",
    "    print(\"Precision:\", precision_m)\n",
    "\n",
    "    # Recall\n",
    "    total_recall = 0\n",
    "    denum = non_empty_label\n",
    "    for i in label_list: # labels are 1, 2, 3\n",
    "        num = recall(result, i)\n",
    "        if num != -1:\n",
    "            total_recall += num\n",
    "        else:\n",
    "            print(\"p_actual is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "            denum -= 1\n",
    "    recall_m = total_recall / non_empty_label\n",
    "    total_recall_after_iteration += recall_m\n",
    "    print(\"Recall:\", recall_m)\n",
    "\n",
    "    # F-score\n",
    "    f_score = 2 * precision_m * recall_m / (precision_m + recall_m)\n",
    "    total_f_score_after_iteration += f_score\n",
    "    print(\"Macro-averaging F-score:\", f_score)\n",
    "    print()\n",
    "\n",
    "print(\"Summary\")\n",
    "print(\"--------------------\")\n",
    "print(\"Average accuracy after 3 iterations:\", total_accuracy_after_iteration / 3)\n",
    "print(\"Average precision after 3 iterations:\", total_precision_after_iteration / 3)\n",
    "print(\"Average recall after 3 iterations:\", total_recall_after_iteration / 3)\n",
    "print(\"Average F-score after 3 iterations:\", total_f_score_after_iteration / 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f578cb",
   "metadata": {},
   "source": [
    "Comparisons\n",
    "\n",
    "The results are as follow:\n",
    "|Metrics|bin=4|bin=5|\n",
    "|---|---|---|\n",
    "|Accuracy|0.8181|0.8475|\n",
    "|Precision|0.4586|0.4251|\n",
    "|Recall|0.6025|0.6468|\n",
    "|F-score|0.5199|0.5103|\n",
    "\n",
    "* Accuracy is higher at bin=5 than bin=4.\n",
    "* Precision is higher at bin=4 than bin=5, though not significantly.\n",
    "* Recall is higher at bin=5 than bin=4.\n",
    "* F-score is higher at bin=4 than bin=5, though not significantly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047a517b",
   "metadata": {},
   "source": [
    "### 2.11 Redo 2.6 and compare the results at bin=4 and two different k values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b546d1",
   "metadata": {},
   "source": [
    "At k=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da4498a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "--------------------\n",
      "actual      1    2    3    4    total\n",
      "--------  ---  ---  ---  ---  -------\n",
      "1          50    9    0    0       59\n",
      "2           9   29   32    0       70\n",
      "3           0    2   18    0       20\n",
      "4           0    0    4    0        4\n",
      "total      59   40   54    0      153\n",
      "\n",
      "Non-empty labels count: 4\n",
      "Average accuracy: 0.8169934640522876\n",
      "p_prediced is 0 for label 4 . Subtracted 1 from non_empty_label.\n",
      "Precision: 0.47644774011299434\n",
      "Recall: 0.7205811138014528\n",
      "Macro-averaging F-score: 0.5736189935875079\n",
      "\n",
      "Iteration 2\n",
      "--------------------\n",
      "actual      1    2    3    4    total\n",
      "--------  ---  ---  ---  ---  -------\n",
      "1          38    7    0    0       45\n",
      "2           3   37   31    0       71\n",
      "3           0    5   27    0       32\n",
      "4           0    0    5    0        5\n",
      "total      41   49   63    0      153\n",
      "\n",
      "Non-empty labels count: 4\n",
      "Average accuracy: 0.8333333333333334\n",
      "p_prediced is 0 for label 4 . Subtracted 1 from non_empty_label.\n",
      "Precision: 0.5276256844201095\n",
      "Recall: 0.7364404016692748\n",
      "Macro-averaging F-score: 0.6147856907821438\n",
      "\n",
      "Iteration 3\n",
      "--------------------\n",
      "actual      1    2    3    4    total\n",
      "--------  ---  ---  ---  ---  -------\n",
      "1          47    6    0    0       53\n",
      "2           5   35   27    0       67\n",
      "3           0    1   28    0       29\n",
      "4           0    0    4    0        4\n",
      "total      52   42   59    0      153\n",
      "\n",
      "Non-empty labels count: 4\n",
      "Average accuracy: 0.8594771241830066\n",
      "p_prediced is 0 for label 4 . Subtracted 1 from non_empty_label.\n",
      "Precision: 0.552938939591482\n",
      "Recall: 0.7915659179703306\n",
      "Macro-averaging F-score: 0.6510762930124263\n",
      "\n",
      "Summary\n",
      "--------------------\n",
      "Average accuracy after 3 iterations: 0.8366013071895425\n",
      "Average precision after 3 iterations: 0.519004121374862\n",
      "Average recall after 3 iterations: 0.7495291444803528\n",
      "Average F-score after 3 iterations: 0.613160325794026\n"
     ]
    }
   ],
   "source": [
    "total_accuracy_after_iteration = 0\n",
    "total_precision_after_iteration = 0\n",
    "total_recall_after_iteration = 0\n",
    "total_f_score_after_iteration = 0\n",
    "\n",
    "for i in range(3):\n",
    "    # Discretize\n",
    "    auto_copy = auto.copy()\n",
    "    mpg_value = auto_copy.get_column_data('mpg')\n",
    "    mpg_value = sorted(mpg_value)\n",
    "    bin_width = (mpg_value[-1] - mpg_value[0]) / 4\n",
    "    cut_points = [mpg_value[0] + bin_width, mpg_value[0] + 2 * bin_width, mpg_value[0] + 3 * bin_width]\n",
    "    discretize(auto_copy, 'mpg', cut_points)\n",
    "\n",
    "    # Normalize\n",
    "    normalize(auto_copy, 'cyls')\n",
    "    normalize(auto_copy, 'disp')\n",
    "    normalize(auto_copy, 'hp')\n",
    "    normalize(auto_copy, 'weight')\n",
    "    normalize(auto_copy, 'accl')\n",
    "\n",
    "    # Holdout\n",
    "    train, test = holdout(auto_copy, auto_copy.row_count()//2)\n",
    "\n",
    "    # kNN\n",
    "    result = knn_eval(train, test, majority_vote, 10, 'mpg', ['cyls','weight','accl'], [])\n",
    "    label_list = result.get_column_data('actual')\n",
    "    if 'total' in label_list:\n",
    "        label_list.remove('total')\n",
    "\n",
    "    print(\"Iteration\", i+1)\n",
    "    print(\"--------------------\")\n",
    "    print(result)\n",
    "    print()\n",
    "\n",
    "    # Find non-empty labels\n",
    "    sum_of_actual_labels = result.get_column_data('total')\n",
    "    non_empty_label = 0\n",
    "    for i in sum_of_actual_labels:\n",
    "        if i != 0:\n",
    "            non_empty_label += 1\n",
    "\n",
    "    if result[result.row_count() - 1]['actual'] == 'total':\n",
    "        non_empty_label -= 1\n",
    "\n",
    "    print(\"Non-empty labels count:\", non_empty_label)\n",
    "\n",
    "    # Accuracy\n",
    "    total_accuracy = 0\n",
    "    for i in label_list: \n",
    "        num = accuracy(result, i)\n",
    "        if num != -1:\n",
    "            total_accuracy += num\n",
    "    acc = total_accuracy / non_empty_label\n",
    "    total_accuracy_after_iteration += acc\n",
    "    print(\"Average accuracy:\", acc)\n",
    "\n",
    "    # Precision\n",
    "    total_precision = 0\n",
    "    denum = non_empty_label\n",
    "    for i in label_list:\n",
    "        num = precision(result, i)\n",
    "        if num != -1:\n",
    "            total_precision += num\n",
    "        else:\n",
    "            print(\"p_prediced is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "            non_empty_label -= 1\n",
    "    precision_m = total_precision / denum\n",
    "    total_precision_after_iteration += precision_m\n",
    "    print(\"Precision:\", precision_m)\n",
    "\n",
    "    # Recall\n",
    "    total_recall = 0\n",
    "    denum = non_empty_label\n",
    "    for i in label_list: # labels are 1, 2, 3\n",
    "        num = recall(result, i)\n",
    "        if num != -1:\n",
    "            total_recall += num\n",
    "        else:\n",
    "            print(\"p_actual is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "            denum -= 1\n",
    "    recall_m = total_recall / non_empty_label\n",
    "    total_recall_after_iteration += recall_m\n",
    "    print(\"Recall:\", recall_m)\n",
    "\n",
    "    # F-score\n",
    "    f_score = 2 * precision_m * recall_m / (precision_m + recall_m)\n",
    "    total_f_score_after_iteration += f_score\n",
    "    print(\"Macro-averaging F-score:\", f_score)\n",
    "    print()\n",
    "\n",
    "print(\"Summary\")\n",
    "print(\"--------------------\")\n",
    "print(\"Average accuracy after 3 iterations:\", total_accuracy_after_iteration / 3)\n",
    "print(\"Average precision after 3 iterations:\", total_precision_after_iteration / 3)\n",
    "print(\"Average recall after 3 iterations:\", total_recall_after_iteration / 3)\n",
    "print(\"Average F-score after 3 iterations:\", total_f_score_after_iteration / 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1240e77",
   "metadata": {},
   "source": [
    "At k=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cb34297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "--------------------\n",
      "actual      1    2    3    4    total\n",
      "--------  ---  ---  ---  ---  -------\n",
      "1          39   11    0    0       50\n",
      "2           6   26   40    0       72\n",
      "3           0    1   26    0       27\n",
      "4           0    0    4    0        4\n",
      "total      45   38   70    0      153\n",
      "\n",
      "Non-empty labels count: 4\n",
      "Average accuracy: 0.7973856209150327\n",
      "p_prediced is 0 for label 4 . Subtracted 1 from non_empty_label.\n",
      "Precision: 0.4805764411027569\n",
      "Recall: 0.7013580246913581\n",
      "Macro-averaging F-score: 0.5703465855335254\n",
      "\n",
      "Iteration 2\n",
      "--------------------\n",
      "actual      1    2    3    4    total\n",
      "--------  ---  ---  ---  ---  -------\n",
      "1          40    9    0    0       49\n",
      "2           7   38   26    0       71\n",
      "3           0    3   25    0       28\n",
      "4           0    0    5    0        5\n",
      "total      47   50   56    0      153\n",
      "\n",
      "Non-empty labels count: 4\n",
      "Average accuracy: 0.8366013071895425\n",
      "p_prediced is 0 for label 4 . Subtracted 1 from non_empty_label.\n",
      "Precision: 0.5143731003039514\n",
      "Recall: 0.7481316470250072\n",
      "Macro-averaging F-score: 0.6096116399243701\n",
      "\n",
      "Iteration 3\n",
      "--------------------\n",
      "actual      1    2    3    4    total\n",
      "--------  ---  ---  ---  ---  -------\n",
      "1          35   12    0    0       47\n",
      "2          11   58    0    0       69\n",
      "3           0   34    0    0       34\n",
      "4           0    3    0    0        3\n",
      "total      46  107    0    0      153\n",
      "\n",
      "Non-empty labels count: 4\n",
      "Average accuracy: 0.803921568627451\n",
      "p_prediced is 0 for label 3 . Subtracted 1 from non_empty_label.\n",
      "p_prediced is 0 for label 4 . Subtracted 1 from non_empty_label.\n",
      "Precision: 0.32573140999593664\n",
      "Recall: 0.7926302806043786\n",
      "Macro-averaging F-score: 0.4617192829059632\n",
      "\n",
      "Summary\n",
      "--------------------\n",
      "Average accuracy after 3 iterations: 0.8126361655773421\n",
      "Average precision after 3 iterations: 0.44022698380088165\n",
      "Average recall after 3 iterations: 0.7473733174402479\n",
      "Average F-score after 3 iterations: 0.5472258361212862\n"
     ]
    }
   ],
   "source": [
    "total_accuracy_after_iteration = 0\n",
    "total_precision_after_iteration = 0\n",
    "total_recall_after_iteration = 0\n",
    "total_f_score_after_iteration = 0\n",
    "\n",
    "for i in range(3):\n",
    "    # Discretize\n",
    "    auto_copy = auto.copy()\n",
    "    mpg_value = auto_copy.get_column_data('mpg')\n",
    "    mpg_value = sorted(mpg_value)\n",
    "    bin_width = (mpg_value[-1] - mpg_value[0]) / 4\n",
    "    cut_points = [mpg_value[0] + bin_width, mpg_value[0] + 2 * bin_width, mpg_value[0] + 3 * bin_width]\n",
    "    discretize(auto_copy, 'mpg', cut_points)\n",
    "\n",
    "    # Normalize\n",
    "    normalize(auto_copy, 'cyls')\n",
    "    normalize(auto_copy, 'disp')\n",
    "    normalize(auto_copy, 'hp')\n",
    "    normalize(auto_copy, 'weight')\n",
    "    normalize(auto_copy, 'accl')\n",
    "\n",
    "    # Holdout\n",
    "    train, test = holdout(auto_copy, auto_copy.row_count()//2)\n",
    "\n",
    "    # kNN\n",
    "    result = knn_eval(train, test, majority_vote, 20, 'mpg', ['cyls','weight','accl'], [])\n",
    "    label_list = result.get_column_data('actual')\n",
    "    if 'total' in label_list:\n",
    "        label_list.remove('total')\n",
    "\n",
    "    print(\"Iteration\", i+1)\n",
    "    print(\"--------------------\")\n",
    "    print(result)\n",
    "    print()\n",
    "\n",
    "    # Find non-empty labels\n",
    "    sum_of_actual_labels = result.get_column_data('total')\n",
    "    non_empty_label = 0\n",
    "    for i in sum_of_actual_labels:\n",
    "        if i != 0:\n",
    "            non_empty_label += 1\n",
    "\n",
    "    if result[result.row_count() - 1]['actual'] == 'total':\n",
    "        non_empty_label -= 1\n",
    "\n",
    "    print(\"Non-empty labels count:\", non_empty_label)\n",
    "\n",
    "    # Accuracy\n",
    "    total_accuracy = 0\n",
    "    for i in label_list: \n",
    "        num = accuracy(result, i)\n",
    "        if num != -1:\n",
    "            total_accuracy += num\n",
    "    acc = total_accuracy / non_empty_label\n",
    "    total_accuracy_after_iteration += acc\n",
    "    print(\"Average accuracy:\", acc)\n",
    "\n",
    "    # Precision\n",
    "    total_precision = 0\n",
    "    denum = non_empty_label\n",
    "    for i in label_list:\n",
    "        num = precision(result, i)\n",
    "        if num != -1:\n",
    "            total_precision += num\n",
    "        else:\n",
    "            print(\"p_prediced is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "            non_empty_label -= 1\n",
    "    precision_m = total_precision / denum\n",
    "    total_precision_after_iteration += precision_m\n",
    "    print(\"Precision:\", precision_m)\n",
    "\n",
    "    # Recall\n",
    "    total_recall = 0\n",
    "    denum = non_empty_label\n",
    "    for i in label_list: # labels are 1, 2, 3\n",
    "        num = recall(result, i)\n",
    "        if num != -1:\n",
    "            total_recall += num\n",
    "        else:\n",
    "            print(\"p_actual is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "            denum -= 1\n",
    "    recall_m = total_recall / non_empty_label\n",
    "    total_recall_after_iteration += recall_m\n",
    "    print(\"Recall:\", recall_m)\n",
    "\n",
    "    # F-score\n",
    "    f_score = 2 * precision_m * recall_m / (precision_m + recall_m)\n",
    "    total_f_score_after_iteration += f_score\n",
    "    print(\"Macro-averaging F-score:\", f_score)\n",
    "    print()\n",
    "\n",
    "print(\"Summary\")\n",
    "print(\"--------------------\")\n",
    "print(\"Average accuracy after 3 iterations:\", total_accuracy_after_iteration / 3)\n",
    "print(\"Average precision after 3 iterations:\", total_precision_after_iteration / 3)\n",
    "print(\"Average recall after 3 iterations:\", total_recall_after_iteration / 3)\n",
    "print(\"Average F-score after 3 iterations:\", total_f_score_after_iteration / 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08a2797",
   "metadata": {},
   "source": [
    "Comparisons\n",
    "\n",
    "The results are as follow:\n",
    "|Metrics|k=10|k=20|\n",
    "|---|---|---|\n",
    "|Accuracy|0.8366|0.8126|\n",
    "|Precision|0.5190|0.4402|\n",
    "|Recall|0.7495|0.7474|\n",
    "|F-score|0.6132|0.5492|\n",
    "\n",
    "* Accuracy is higher at k=10 than k=20, though not significantly.\n",
    "* Precision is higher at k=10 than k=20. The gap is quite big, meaning when k is higher, the precision is lower. This may also suggest that the optimal k value is between 10 and 20.\n",
    "* Recall is higher at k=10 than k=20, though slightly.\n",
    "* F-score is higher at k=10 than k=20. This also suggests the same thing as the precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6390bb",
   "metadata": {},
   "source": [
    "## 3. Issues, Challenges, and Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf6b940",
   "metadata": {},
   "source": [
    "### 3.1 Issues and Challenges\n",
    "* I don't have any significant issues completing this assignment.\n",
    "* I was stuck at knn_eval() since I did not know what to expect from the returning result. Besides, I did not think about dropping the label column before passing the training set to calculate the Eucledian distance for KNN. Therefore, it took me a while to figure out what to do and debug the code.\n",
    "\n",
    "### 3.2 Observations\n",
    "* I notice a higher score when using majority vote over weighted vote. This is interesting as I thought weighted vote would be better since it takes into account the distance between the points. This might be casued by the amount of data is not that sufficient to make a difference. Besides, since I also scaled the scores that were used for the weighted vote, it might have caused the result to be lower.\n",
    "* For this set of data, I think the optimal equal-width bin size is 3. At bin=3, the accuracy and f-score yield the highest as compared to bin=4 or bin=5. Otherwise, if we use other discretization methods that can preserve the distribution, the result might be better.\n",
    "* The optimal k value is between 10 and 20. At k=10, the results are better at k=5 and k=20. We can put this into a loop and run, while comparing the results to find the optimal k value.\n",
    "* The Holdout method is not that good as it does not maintain the distribution. This is interesting as in some extreme cases, it may produce a test set that does not contain every label. Therefore, the result might be biased. We can use other methods such as k-fold cross validation to get a better result.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
