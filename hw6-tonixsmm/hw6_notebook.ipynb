{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c50355",
   "metadata": {},
   "source": [
    "# HW6 Notebook\n",
    "Name: **Tony Nguyen**\n",
    "\n",
    "Class: CPSC 322, \n",
    "\n",
    "Term: Fall 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084b3952",
   "metadata": {},
   "source": [
    "# 1. Load libraries and datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05155db4",
   "metadata": {},
   "source": [
    " Import the data table and utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed147760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_table import *\n",
    "from data_learn import *\n",
    "from data_eval import *\n",
    "from data_util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6208afb",
   "metadata": {},
   "source": [
    "# 2. Auto MPG Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11de2c0",
   "metadata": {},
   "source": [
    "Load and clean auto data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0decd23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  mpg    cyls    disp    hp    weight    accl    year    origin  name\n",
       "-----  ------  ------  ----  --------  ------  ------  --------  -------------------------\n",
       "   18       8     307   130      3504    12        70         1  chevrolet chevelle malibu\n",
       "   15       8     350   165      3693    11.5      70         1  buick skylark 320\n",
       "   18       8     318   150      3436    11        70         1  plymouth satellite\n",
       "   16       8     304   150      3433    12        70         1  amc rebel sst\n",
       "   17       8     302   140      3449    10.5      70         1  ford torino\n",
       "   15       8     429   198      4341    10        70         1  ford galaxie 500\n",
       "   14       8     454   220      4354     9        70         1  chevrolet impala\n",
       "   14       8     440   215      4312     8.5      70         1  plymouth fury iii\n",
       "   14       8     455   225      4425    10        70         1  pontiac catalina\n",
       "   15       8     390   190      3850     8.5      70         1  amc ambassador dpl"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto = DataTable(['mpg','cyls','disp','hp','weight','accl','year','origin','name'])\n",
    "auto.load('auto-mpg.txt')\n",
    "auto.rows(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before cleaning: 317\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows before cleaning:\", auto.row_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after removing duplicated rows: 316\n"
     ]
    }
   ],
   "source": [
    "auto = remove_duplicates(auto)\n",
    "print(\"Number of rows after removing duplicated rows:\", auto.row_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after removing missing values: 307\n"
     ]
    }
   ],
   "source": [
    "auto = remove_missing(auto, auto.columns())\n",
    "print(\"Number of rows after removing missing values:\", auto.row_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92f7bad",
   "metadata": {},
   "source": [
    "## Step 1: k-NN versus Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Discretize the mpg value in the auto table using three equal-width bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_copy = auto.copy()\n",
    "mpg_value = auto_copy.get_column_data('mpg')\n",
    "mpg_value = sorted(mpg_value)\n",
    "bin_width = (mpg_value[-1] - mpg_value[0]) / 3\n",
    "cut_points = [mpg_value[0] + bin_width, mpg_value[0] + 2 * bin_width]\n",
    "discretize(auto_copy, 'mpg', cut_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  mpg    cyls    disp    hp    weight    accl    year    origin  name\n",
       "-----  ------  ------  ----  --------  ------  ------  --------  ----------------------------\n",
       "    1       8     307   130      3504    12        70         1  chevrolet chevelle malibu\n",
       "    1       8     350   165      3693    11.5      70         1  buick skylark 320\n",
       "    1       8     318   150      3436    11        70         1  plymouth satellite\n",
       "    1       8     304   150      3433    12        70         1  amc rebel sst\n",
       "    1       8     302   140      3449    10.5      70         1  ford torino\n",
       "    1       8     429   198      4341    10        70         1  ford galaxie 500\n",
       "    1       8     454   220      4354     9        70         1  chevrolet impala\n",
       "    1       8     440   215      4312     8.5      70         1  plymouth fury iii\n",
       "    1       8     455   225      4425    10        70         1  pontiac catalina\n",
       "    1       8     390   190      3850     8.5      70         1  amc ambassador dpl\n",
       "    1       8     383   170      3563    10        70         1  dodge challenger se\n",
       "    1       8     340   160      3609     8        70         1  plymouth 'cuda 340\n",
       "    1       8     400   150      3761     9.5      70         1  chevrolet monte carlo s\n",
       "    1       8     455   225      3086    10        70         1  buick estate wagon (sw)\n",
       "    2       4     113    95      2372    15        70         3  toyota corona mark ii\n",
       "    2       6     198    95      2833    15.5      70         1  plymouth duster\n",
       "    1       6     199    97      2774    15.5      70         1  amc hornet\n",
       "    2       6     200    85      2587    16        70         1  ford maverick\n",
       "    2       4      97    88      2130    14.5      70         3  datsun pl510\n",
       "    2       4      97    46      1835    20.5      70         2  volkswagen 1131 deluxe sedan"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_copy.rows(range(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Normalize the weight (weight) and displacement (disp) attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize(auto_copy, 'weight')\n",
    "normalize(auto_copy, 'disp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Evaluate knn using stratified k-fold cross validation (i.e., your knn_stratified() function) to predict mpg labels using 10 folds, a knn k-value of 7, majority voting, and only the weight and displacement attributes (as numeric columns). Display the resulting confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual      1    2    3    total\n",
      "--------  ---  ---  ---  -------\n",
      "1         162    0    0      162\n",
      "2         123    0    0      123\n",
      "3          22    0    0       22\n",
      "total     307    0    0      307\n"
     ]
    }
   ],
   "source": [
    "result = knn_stratified(auto_copy, 10, 'mpg', majority_vote, 7, ['weight', 'disp'])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Compute accuracy, precision, recall, and the f-measure over the resulting confusion matrix and display each.\n",
    "\n",
    "Number of non-empty classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-empty labels count: 3\n"
     ]
    }
   ],
   "source": [
    "sum_of_actual_labels = result.get_column_data('total')\n",
    "non_empty_label = 0\n",
    "for i in sum_of_actual_labels:\n",
    "    if i != 0:\n",
    "        non_empty_label += 1\n",
    "\n",
    "if result[result.row_count() - 1]['actual'] == 'total':\n",
    "    non_empty_label -= 1\n",
    "\n",
    "print(\"Non-empty labels count:\", non_empty_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.6851248642779587\n"
     ]
    }
   ],
   "source": [
    "total_accuracy = 0\n",
    "total_accuracy_after_iteration = 0\n",
    "label_list = result.get_column_data('actual')\n",
    "if label_list[-1] == 'total':\n",
    "    label_list = label_list[:-1]\n",
    "for i in label_list: \n",
    "    num = accuracy(result, i)\n",
    "    if num != -1:\n",
    "        total_accuracy += num\n",
    "acc = total_accuracy / non_empty_label\n",
    "total_accuracy_after_iteration += acc\n",
    "print(\"Average accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_prediced is 0 for label 2 . Subtracted 1 from non_empty_label.\n",
      "p_prediced is 0 for label 3 . Subtracted 1 from non_empty_label.\n",
      "Precision: 0.1758957654723127\n"
     ]
    }
   ],
   "source": [
    "total_precision = 0\n",
    "denum = non_empty_label\n",
    "for i in label_list:\n",
    "    num = precision(result, i)\n",
    "    if num != -1:\n",
    "        total_precision += num\n",
    "    else:\n",
    "        print(\"p_prediced is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "        non_empty_label -= 1\n",
    "precision_m = total_precision / denum\n",
    "print(\"Precision:\", precision_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "total_recall = 0\n",
    "denum = non_empty_label\n",
    "for i in label_list: # labels are 1, 2, 3\n",
    "    num = recall(result, i)\n",
    "    if num != -1:\n",
    "        total_recall += num\n",
    "    else:\n",
    "        print(\"p_actual is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "        denum -= 1\n",
    "recall_m = total_recall / non_empty_label\n",
    "print(\"Recall:\", recall_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find f-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-averaging F-score: 0.29916897506925205\n"
     ]
    }
   ],
   "source": [
    "f_score = 2 * precision_m * recall_m / (precision_m + recall_m)\n",
    "print(\"Macro-averaging F-score:\", f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Repeat step 3 using the same parameters but using naive-bayes instead of knn (i.e., your naive_bayes_stratified() function). Be sure to use weight and displacement as continuous attributes. Display the resulting confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = naive_bayes_stratified(auto_copy, 10, 'mpg', ['weight', 'disp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual      1    2    3    total\n",
      "--------  ---  ---  ---  -------\n",
      "1         143   18    1      162\n",
      "2          14   77   32      123\n",
      "3           0    5   17       22\n",
      "total     157  100   50      307\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Compute accraccy, precision, recall, and the f-measure over the resulting confusion matrix and display each.\n",
    "\n",
    "Number of non-empty classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-empty labels count: 3\n"
     ]
    }
   ],
   "source": [
    "sum_of_actual_labels = result.get_column_data('total')\n",
    "non_empty_label = 0\n",
    "for i in sum_of_actual_labels:\n",
    "    if i != 0:\n",
    "        non_empty_label += 1\n",
    "\n",
    "if result[result.row_count() - 1]['actual'] == 'total':\n",
    "    non_empty_label -= 1\n",
    "\n",
    "print(\"Non-empty labels count:\", non_empty_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy: 0.8479913137893593\n"
     ]
    }
   ],
   "source": [
    "total_accuracy = 0\n",
    "total_accuracy_after_iteration = 0\n",
    "label_list = result.get_column_data('actual')\n",
    "if label_list[-1] == 'total':\n",
    "    label_list = label_list[:-1]\n",
    "for i in label_list: \n",
    "    num = accuracy(result, i)\n",
    "    if num != -1:\n",
    "        total_accuracy += num\n",
    "acc = total_accuracy / non_empty_label\n",
    "total_accuracy_after_iteration += acc\n",
    "print(\"Average accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6736093418259023\n"
     ]
    }
   ],
   "source": [
    "total_precision = 0\n",
    "denum = non_empty_label\n",
    "for i in label_list:\n",
    "    num = precision(result, i)\n",
    "    if num != -1:\n",
    "        total_precision += num\n",
    "    else:\n",
    "        print(\"p_prediced is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "        non_empty_label -= 1\n",
    "precision_m = total_precision / denum\n",
    "print(\"Precision:\", precision_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.7604865274241966\n"
     ]
    }
   ],
   "source": [
    "total_recall = 0\n",
    "denum = non_empty_label\n",
    "for i in label_list: # labels are 1, 2, 3\n",
    "    num = recall(result, i)\n",
    "    if num != -1:\n",
    "        total_recall += num\n",
    "    else:\n",
    "        print(\"p_actual is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "        denum -= 1\n",
    "recall_m = total_recall / non_empty_label\n",
    "print(\"Recall:\", recall_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find f-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-averaging F-score: 0.7144164350372894\n"
     ]
    }
   ],
   "source": [
    "f_score = 2 * precision_m * recall_m / (precision_m + recall_m)\n",
    "print(\"Macro-averaging F-score:\", f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Comparing knn and naive-bayes stratified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, the differences between the two is quite significant.\n",
    "* The accuracy of knn is 0.68 while the accuracy of naive-bayes is 0.85.\n",
    "* The precision of knn is 0.18 while the precision of naive-bayes is 0.67.\n",
    "* The recall of knn is 1 while the recall of naive-bayes is 0.76.\n",
    "* The f-measure of knn is 0.3 while the f-measure of naive-bayes is 0.71.\n",
    "\n",
    "I find the main reason that makes such a different result is because of the k-stratified cross validation selection method.\n",
    "* For some reason, while it was picking samples for the knn test, it was picking samples that were not representative of the whole dataset.\n",
    "* Although the test set preserved the original class distribution, it might not be enough to produce a good test result as it was not representative of the whole dataset.\n",
    "* Thus, the model for knn was underfitting the data and produced a bad result.\n",
    "* Naive-bayes produced a much better result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424e4dbb",
   "metadata": {},
   "source": [
    "## Step 2: Experimentation with Auto MPG Data Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Try to factor in more attributes to see if the results improve.\n",
    "\n",
    "kNN result\n",
    "* Pick k_fold=10, k=20 and bin=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual      1    2    3    total\n",
      "--------  ---  ---  ---  -------\n",
      "1         149   13    0      162\n",
      "2          23   97    3      123\n",
      "3           0   20    2       22\n",
      "total     172  130    5      307\n",
      "\n",
      "Non-empty labels count: 3\n",
      "Average accuracy: 0.8718783930510314\n",
      "Precision: 0.6708109719737627\n",
      "Recall: 0.5997600211692352\n",
      "Macro-averaging F-score: 0.6332988946273842\n"
     ]
    }
   ],
   "source": [
    "# Discretize\n",
    "auto_copy = auto.copy()\n",
    "mpg_value = auto_copy.get_column_data('mpg')\n",
    "mpg_value = sorted(mpg_value)\n",
    "bin_width = (mpg_value[-1] - mpg_value[0]) / 3\n",
    "cut_points = [mpg_value[0] + bin_width, mpg_value[0] + 2 * bin_width]\n",
    "discretize(auto_copy, 'mpg', cut_points)\n",
    "\n",
    "# Normalize\n",
    "normalize(auto_copy, 'cyls')\n",
    "normalize(auto_copy, 'disp')\n",
    "normalize(auto_copy, 'hp')\n",
    "normalize(auto_copy, 'weight')\n",
    "normalize(auto_copy, 'accl')\n",
    "\n",
    "\n",
    "# kNN\n",
    "result = knn_stratified(auto_copy, 10, 'mpg', majority_vote, 20, ['cyls', 'disp', 'hp', 'weight', 'accl'])\n",
    "print(result)\n",
    "print()\n",
    "label_list = result.get_column_data('actual')\n",
    "if 'total' in label_list:\n",
    "    label_list.remove('total')\n",
    "\n",
    "# Find non-empty labels\n",
    "sum_of_actual_labels = result.get_column_data('total')\n",
    "non_empty_label = 0\n",
    "for i in sum_of_actual_labels:\n",
    "    if i != 0:\n",
    "        non_empty_label += 1\n",
    "\n",
    "if result[result.row_count() - 1]['actual'] == 'total':\n",
    "    non_empty_label -= 1\n",
    "\n",
    "print(\"Non-empty labels count:\", non_empty_label)\n",
    "\n",
    "# Accuracy\n",
    "total_accuracy = 0\n",
    "for i in label_list: \n",
    "    num = accuracy(result, i)\n",
    "    if num != -1:\n",
    "        total_accuracy += num\n",
    "acc = total_accuracy / non_empty_label\n",
    "print(\"Average accuracy:\", acc)\n",
    "\n",
    "# Precision\n",
    "total_precision = 0\n",
    "denum = non_empty_label\n",
    "for i in label_list:\n",
    "    num = precision(result, i)\n",
    "    if num != -1:\n",
    "        total_precision += num\n",
    "    else:\n",
    "        print(\"p_prediced is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "        non_empty_label -= 1\n",
    "precision_m = total_precision / denum\n",
    "print(\"Precision:\", precision_m)\n",
    "\n",
    "# Recall\n",
    "total_recall = 0\n",
    "denum = non_empty_label\n",
    "for i in label_list: # labels are 1, 2, 3\n",
    "    num = recall(result, i)\n",
    "    if num != -1:\n",
    "        total_recall += num\n",
    "    else:\n",
    "        print(\"p_actual is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "        denum -= 1\n",
    "recall_m = total_recall / non_empty_label\n",
    "print(\"Recall:\", recall_m)\n",
    "\n",
    "# F-score\n",
    "f_score = 2 * precision_m * recall_m / (precision_m + recall_m)\n",
    "print(\"Macro-averaging F-score:\", f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it again with Naive Bayes\n",
    "* k=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual      1    2    3    total\n",
      "--------  ---  ---  ---  -------\n",
      "1         145    9    8      162\n",
      "2          12   12   99      123\n",
      "3           0    1   21       22\n",
      "total     157   22  128      307\n",
      "\n",
      "Non-empty labels count: 3\n",
      "Average accuracy: 0.7198697068403909\n",
      "Precision: 0.5443613081451457\n",
      "Recall: 0.6490560528500908\n",
      "Macro-averaging F-score: 0.5921164104640683\n"
     ]
    }
   ],
   "source": [
    "# Discretize\n",
    "auto_copy = auto.copy()\n",
    "mpg_value = auto_copy.get_column_data('mpg')\n",
    "mpg_value = sorted(mpg_value)\n",
    "bin_width = (mpg_value[-1] - mpg_value[0]) / 3\n",
    "cut_points = [mpg_value[0] + bin_width, mpg_value[0] + 2 * bin_width]\n",
    "discretize(auto_copy, 'mpg', cut_points)\n",
    "\n",
    "# Normalize\n",
    "normalize(auto_copy, 'cyls')\n",
    "normalize(auto_copy, 'disp')\n",
    "normalize(auto_copy, 'hp')\n",
    "normalize(auto_copy, 'weight')\n",
    "normalize(auto_copy, 'accl')\n",
    "\n",
    "\n",
    "# kNN\n",
    "result = naive_bayes_stratified(auto_copy, 10, 'mpg', ['cyls', 'disp', 'hp', 'weight', 'accl'])\n",
    "print(result)\n",
    "print()\n",
    "label_list = result.get_column_data('actual')\n",
    "if 'total' in label_list:\n",
    "    label_list.remove('total')\n",
    "\n",
    "# Find non-empty labels\n",
    "sum_of_actual_labels = result.get_column_data('total')\n",
    "non_empty_label = 0\n",
    "for i in sum_of_actual_labels:\n",
    "    if i != 0:\n",
    "        non_empty_label += 1\n",
    "\n",
    "if result[result.row_count() - 1]['actual'] == 'total':\n",
    "    non_empty_label -= 1\n",
    "\n",
    "print(\"Non-empty labels count:\", non_empty_label)\n",
    "\n",
    "# Accuracy\n",
    "total_accuracy = 0\n",
    "for i in label_list: \n",
    "    num = accuracy(result, i)\n",
    "    if num != -1:\n",
    "        total_accuracy += num\n",
    "acc = total_accuracy / non_empty_label\n",
    "print(\"Average accuracy:\", acc)\n",
    "\n",
    "# Precision\n",
    "total_precision = 0\n",
    "denum = non_empty_label\n",
    "for i in label_list:\n",
    "    num = precision(result, i)\n",
    "    if num != -1:\n",
    "        total_precision += num\n",
    "    else:\n",
    "        print(\"p_prediced is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "        non_empty_label -= 1\n",
    "precision_m = total_precision / denum\n",
    "print(\"Precision:\", precision_m)\n",
    "\n",
    "# Recall\n",
    "total_recall = 0\n",
    "denum = non_empty_label\n",
    "for i in label_list: # labels are 1, 2, 3\n",
    "    num = recall(result, i)\n",
    "    if num != -1:\n",
    "        total_recall += num\n",
    "    else:\n",
    "        print(\"p_actual is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "        denum -= 1\n",
    "recall_m = total_recall / non_empty_label\n",
    "print(\"Recall:\", recall_m)\n",
    "\n",
    "# F-score\n",
    "f_score = 2 * precision_m * recall_m / (precision_m + recall_m)\n",
    "print(\"Macro-averaging F-score:\", f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Try it again but with weighting in origin and year as well\n",
    "\n",
    "kNN result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual      1    2    3    4    5    total\n",
      "--------  ---  ---  ---  ---  ---  -------\n",
      "1          60   14    0    0    0       74\n",
      "2          17   80   19    1    0      117\n",
      "3           1   15   60    2    0       78\n",
      "4           0    3   10   22    0       35\n",
      "5           0    0    2    1    0        3\n",
      "total      78  112   91   26    0      307\n",
      "\n",
      "Non-empty labels count: 5\n",
      "Average accuracy: 0.8892508143322475\n",
      "p_prediced is 0 for label 5 . Subtracted 1 from non_empty_label.\n",
      "Precision: 0.5978021978021978\n",
      "Recall: 0.7230934230934231\n",
      "Macro-averaging F-score: 0.6545056713087872\n"
     ]
    }
   ],
   "source": [
    "# Discretize\n",
    "auto_copy = auto.copy()\n",
    "mpg_value = auto_copy.get_column_data('mpg')\n",
    "mpg_value = sorted(mpg_value)\n",
    "bin_width = (mpg_value[-1] - mpg_value[0]) / 5\n",
    "cut_points = [mpg_value[0] + bin_width, mpg_value[0] + 2 * bin_width, mpg_value[0] + 3 * bin_width, mpg_value[0] + 4 * bin_width]\n",
    "discretize(auto_copy, 'mpg', cut_points)\n",
    "\n",
    "# Normalize\n",
    "normalize(auto_copy, 'cyls')\n",
    "normalize(auto_copy, 'disp')\n",
    "normalize(auto_copy, 'hp')\n",
    "normalize(auto_copy, 'weight')\n",
    "normalize(auto_copy, 'accl')\n",
    "\n",
    "\n",
    "# kNN\n",
    "result = knn_stratified(auto_copy, 10, 'mpg', majority_vote, 20, ['cyls', 'disp', 'hp', 'weight', 'accl'], ['year', 'origin'])\n",
    "print(result)\n",
    "print()\n",
    "label_list = result.get_column_data('actual')\n",
    "if 'total' in label_list:\n",
    "    label_list.remove('total')\n",
    "\n",
    "# Find non-empty labels\n",
    "sum_of_actual_labels = result.get_column_data('total')\n",
    "non_empty_label = 0\n",
    "for i in sum_of_actual_labels:\n",
    "    if i != 0:\n",
    "        non_empty_label += 1\n",
    "\n",
    "if result[result.row_count() - 1]['actual'] == 'total':\n",
    "    non_empty_label -= 1\n",
    "\n",
    "print(\"Non-empty labels count:\", non_empty_label)\n",
    "\n",
    "# Accuracy\n",
    "total_accuracy = 0\n",
    "for i in label_list: \n",
    "    num = accuracy(result, i)\n",
    "    if num != -1:\n",
    "        total_accuracy += num\n",
    "acc = total_accuracy / non_empty_label\n",
    "print(\"Average accuracy:\", acc)\n",
    "\n",
    "# Precision\n",
    "total_precision = 0\n",
    "denum = non_empty_label\n",
    "for i in label_list:\n",
    "    num = precision(result, i)\n",
    "    if num != -1:\n",
    "        total_precision += num\n",
    "    else:\n",
    "        print(\"p_prediced is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "        non_empty_label -= 1\n",
    "precision_m = total_precision / denum\n",
    "print(\"Precision:\", precision_m)\n",
    "\n",
    "# Recall\n",
    "total_recall = 0\n",
    "denum = non_empty_label\n",
    "for i in label_list: # labels are 1, 2, 3\n",
    "    num = recall(result, i)\n",
    "    if num != -1:\n",
    "        total_recall += num\n",
    "    else:\n",
    "        print(\"p_actual is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "        denum -= 1\n",
    "recall_m = total_recall / non_empty_label\n",
    "print(\"Recall:\", recall_m)\n",
    "\n",
    "# F-score\n",
    "f_score = 2 * precision_m * recall_m / (precision_m + recall_m)\n",
    "print(\"Macro-averaging F-score:\", f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual      1    2    3    4    5    total\n",
      "--------  ---  ---  ---  ---  ---  -------\n",
      "1          68    6    0    0    0       74\n",
      "2          19   74   16    8    0      117\n",
      "3           1    6   21   50    0       78\n",
      "4           0    0    1   32    2       35\n",
      "5           0    0    0    3    0        3\n",
      "total      88   86   38   93    2      307\n",
      "\n",
      "Non-empty labels count: 5\n",
      "Average accuracy: 0.854071661237785\n",
      "Precision: 0.5059819978918175\n",
      "Recall: 0.5469828069828069\n",
      "Macro-averaging F-score: 0.5256841486218317\n"
     ]
    }
   ],
   "source": [
    "# Discretize\n",
    "auto_copy = auto.copy()\n",
    "mpg_value = auto_copy.get_column_data('mpg')\n",
    "mpg_value = sorted(mpg_value)\n",
    "bin_width = (mpg_value[-1] - mpg_value[0]) / 5\n",
    "cut_points = [mpg_value[0] + bin_width, mpg_value[0] + 2 * bin_width, mpg_value[0] + 3 * bin_width, mpg_value[0] + 4 * bin_width]\n",
    "discretize(auto_copy, 'mpg', cut_points)\n",
    "\n",
    "# Normalize\n",
    "normalize(auto_copy, 'cyls')\n",
    "normalize(auto_copy, 'disp')\n",
    "normalize(auto_copy, 'hp')\n",
    "normalize(auto_copy, 'weight')\n",
    "normalize(auto_copy, 'accl')\n",
    "\n",
    "\n",
    "# kNN\n",
    "result = naive_bayes_stratified(auto_copy, 10, 'mpg', ['cyls', 'disp', 'hp', 'weight', 'accl'], ['origin', 'year'])\n",
    "print(result)\n",
    "print()\n",
    "label_list = result.get_column_data('actual')\n",
    "if 'total' in label_list:\n",
    "    label_list.remove('total')\n",
    "\n",
    "# Find non-empty labels\n",
    "sum_of_actual_labels = result.get_column_data('total')\n",
    "non_empty_label = 0\n",
    "for i in sum_of_actual_labels:\n",
    "    if i != 0:\n",
    "        non_empty_label += 1\n",
    "\n",
    "if result[result.row_count() - 1]['actual'] == 'total':\n",
    "    non_empty_label -= 1\n",
    "\n",
    "print(\"Non-empty labels count:\", non_empty_label)\n",
    "\n",
    "# Accuracy\n",
    "total_accuracy = 0\n",
    "for i in label_list: \n",
    "    num = accuracy(result, i)\n",
    "    if num != -1:\n",
    "        total_accuracy += num\n",
    "acc = total_accuracy / non_empty_label\n",
    "print(\"Average accuracy:\", acc)\n",
    "\n",
    "# Precision\n",
    "total_precision = 0\n",
    "denum = non_empty_label\n",
    "for i in label_list:\n",
    "    num = precision(result, i)\n",
    "    if num != -1:\n",
    "        total_precision += num\n",
    "    else:\n",
    "        print(\"p_prediced is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "        non_empty_label -= 1\n",
    "precision_m = total_precision / denum\n",
    "print(\"Precision:\", precision_m)\n",
    "\n",
    "# Recall\n",
    "total_recall = 0\n",
    "denum = non_empty_label\n",
    "for i in label_list: # labels are 1, 2, 3\n",
    "    num = recall(result, i)\n",
    "    if num != -1:\n",
    "        total_recall += num\n",
    "    else:\n",
    "        print(\"p_actual is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "        denum -= 1\n",
    "recall_m = total_recall / non_empty_label\n",
    "print(\"Recall:\", recall_m)\n",
    "\n",
    "# F-score\n",
    "f_score = 2 * precision_m * recall_m / (precision_m + recall_m)\n",
    "print(\"Macro-averaging F-score:\", f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d39f9ee",
   "metadata": {},
   "source": [
    "### 2.3 Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After changing the bin size to 5, the result for kNN improved a lot.\n",
    "* The accuracy went to 0.89, which was a significant improvement from the previous result.\n",
    "* The precision went to 0.60, which was even better than Naive Bayes running at the same parameter.\n",
    "* The pattern repeats itself for recall and f-score.\n",
    "* So, it means that it was not the kNN algorithm that worked poorly, but the parameter size was too small for the algorithm to work properly.\n",
    "* Increasing the sizes of the parameters see a better result, and even more trustworthy than Naive Bayes as its prediction, recall, and f-score were all better than Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b5c32c",
   "metadata": {},
   "source": [
    "# 2. Titanic Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d8cfc4",
   "metadata": {},
   "source": [
    "Load the titanic data set below. The attributes are *class*, *age*, *gender*, and *survival*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2916b12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class    age    gender    survival\n",
       "-------  -----  --------  ----------\n",
       "crew     adult  female    yes\n",
       "first    adult  male      no\n",
       "first    adult  male      no\n",
       "third    adult  female    no\n",
       "second   adult  female    yes\n",
       "third    adult  female    yes\n",
       "second   adult  male      yes\n",
       "second   adult  female    yes\n",
       "crew     child  female    no\n",
       "third    adult  female    no"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic = DataTable(['class', 'age', 'gender', 'survival'])\n",
    "titanic.load('titanic.txt')\n",
    "titanic.rows(range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Classify survival using knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual      no    yes    total\n",
      "--------  ----  -----  -------\n",
      "no           0    711      711\n",
      "yes          0   1490     1490\n",
      "total        0   2201     2201\n",
      "\n",
      "Non-empty labels count: 2\n",
      "Average accuracy: 0.6769650159018628\n",
      "p_prediced is 0 for label no . Subtracted 1 from non_empty_label.\n",
      "Precision: 0.3384825079509314\n",
      "Recall: 1.0\n",
      "Macro-averaging F-score: 0.5057705363204344\n"
     ]
    }
   ],
   "source": [
    "# kNN\n",
    "titanic_copy = titanic.copy()\n",
    "result = knn_stratified(titanic_copy, 10, 'survival', majority_vote, 7, [], ['class', 'age', 'gender'])\n",
    "print(result)\n",
    "print()\n",
    "label_list = result.get_column_data('actual')\n",
    "if 'total' in label_list:\n",
    "    label_list.remove('total')\n",
    "\n",
    "# Find non-empty labels\n",
    "sum_of_actual_labels = result.get_column_data('total')\n",
    "non_empty_label = 0\n",
    "for i in sum_of_actual_labels:\n",
    "    if i != 0:\n",
    "        non_empty_label += 1\n",
    "\n",
    "if result[result.row_count() - 1]['actual'] == 'total':\n",
    "    non_empty_label -= 1\n",
    "\n",
    "print(\"Non-empty labels count:\", non_empty_label)\n",
    "\n",
    "# Accuracy\n",
    "total_accuracy = 0\n",
    "for i in label_list: \n",
    "    num = accuracy(result, i)\n",
    "    if num != -1:\n",
    "        total_accuracy += num\n",
    "acc = total_accuracy / non_empty_label\n",
    "print(\"Average accuracy:\", acc)\n",
    "\n",
    "# Precision\n",
    "total_precision = 0\n",
    "denum = non_empty_label\n",
    "for i in label_list:\n",
    "    num = precision(result, i)\n",
    "    if num != -1:\n",
    "        total_precision += num\n",
    "    else:\n",
    "        print(\"p_prediced is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "        non_empty_label -= 1\n",
    "precision_m = total_precision / denum\n",
    "print(\"Precision:\", precision_m)\n",
    "\n",
    "# Recall\n",
    "total_recall = 0\n",
    "denum = non_empty_label\n",
    "for i in label_list: # labels are 1, 2, 3\n",
    "    num = recall(result, i)\n",
    "    if num != -1:\n",
    "        total_recall += num\n",
    "    else:\n",
    "        print(\"p_actual is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "        denum -= 1\n",
    "recall_m = total_recall / non_empty_label\n",
    "print(\"Recall:\", recall_m)\n",
    "\n",
    "# F-score\n",
    "f_score = 2 * precision_m * recall_m / (precision_m + recall_m)\n",
    "print(\"Macro-averaging F-score:\", f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Classify survival using naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual      no    yes    total\n",
      "--------  ----  -----  -------\n",
      "no         349    362      711\n",
      "yes        126   1364     1490\n",
      "total      475   1726     2201\n",
      "\n",
      "Non-empty labels count: 2\n",
      "Average accuracy: 0.7782825988187188\n",
      "Precision: 0.7625016771360615\n",
      "Recall: 0.7031470940824437\n",
      "Macro-averaging F-score: 0.7316225401880811\n"
     ]
    }
   ],
   "source": [
    "# kNN\n",
    "titanic_copy = titanic.copy()\n",
    "result = naive_bayes_stratified(titanic_copy, 10, 'survival', [], ['class', 'age', 'gender'])\n",
    "print(result)\n",
    "print()\n",
    "label_list = result.get_column_data('actual')\n",
    "if 'total' in label_list:\n",
    "    label_list.remove('total')\n",
    "\n",
    "# Find non-empty labels\n",
    "sum_of_actual_labels = result.get_column_data('total')\n",
    "non_empty_label = 0\n",
    "for i in sum_of_actual_labels:\n",
    "    if i != 0:\n",
    "        non_empty_label += 1\n",
    "\n",
    "if result[result.row_count() - 1]['actual'] == 'total':\n",
    "    non_empty_label -= 1\n",
    "\n",
    "print(\"Non-empty labels count:\", non_empty_label)\n",
    "\n",
    "# Accuracy\n",
    "total_accuracy = 0\n",
    "for i in label_list: \n",
    "    num = accuracy(result, i)\n",
    "    if num != -1:\n",
    "        total_accuracy += num\n",
    "acc = total_accuracy / non_empty_label\n",
    "print(\"Average accuracy:\", acc)\n",
    "\n",
    "# Precision\n",
    "total_precision = 0\n",
    "denum = non_empty_label\n",
    "for i in label_list:\n",
    "    num = precision(result, i)\n",
    "    if num != -1:\n",
    "        total_precision += num\n",
    "    else:\n",
    "        print(\"p_prediced is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "        non_empty_label -= 1\n",
    "precision_m = total_precision / denum\n",
    "print(\"Precision:\", precision_m)\n",
    "\n",
    "# Recall\n",
    "total_recall = 0\n",
    "denum = non_empty_label\n",
    "for i in label_list: # labels are 1, 2, 3\n",
    "    num = recall(result, i)\n",
    "    if num != -1:\n",
    "        total_recall += num\n",
    "    else:\n",
    "        print(\"p_actual is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "        denum -= 1\n",
    "recall_m = total_recall / non_empty_label\n",
    "print(\"Recall:\", recall_m)\n",
    "\n",
    "# F-score\n",
    "f_score = 2 * precision_m * recall_m / (precision_m + recall_m)\n",
    "print(\"Macro-averaging F-score:\", f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Again, performing k-stratified cross validation on naive bayes produced a better result, as the algorithm works better in choosing the test set that works well with the respective algorithm.\n",
    "* However, as seen above, it might be caused by the size parameter of the kNN algorithm. The result can definitely be inmproved if we increase the size of the parameter.\n",
    "* The test set is more representative while performing naive bayes, as shown in the confusion matrix.\n",
    "* In terms of accuracy, both knn and naive bayes produced a pretty decent result, with naive bayes being better at 0.78 and knn at 0.68.\n",
    "* However, as the kNN algorithm is sensitive to skew. As a result, the precision and recall is much lower than naive bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c645af2",
   "metadata": {},
   "source": [
    "# 3. Student Stress Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f68ce9c",
   "metadata": {},
   "source": [
    "Load the student stress data set below. The attributes are given below in column order, where the short name to use is given in parenthesis: \n",
    "1. sleep_quality (sleep)\n",
    "2. living_conditions (living)\n",
    "3. basic_needs (basics)\n",
    "4. academic_performance (academic)\n",
    "5. study_load (study)\n",
    "6. future_career_concerns (career)\n",
    "7. social_support (social)\n",
    "8. extracurricular_activities (extra)\n",
    "9. stress_level (stress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2be851a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  sleep    living    basics    academic    study    career    social    extra    stress\n",
       "-------  --------  --------  ----------  -------  --------  --------  -------  --------\n",
       "      2         3         2           3        2         3         2        3         1\n",
       "      1         1         2           1        4         5         1        5         2\n",
       "      2         2         2           2        3         2         2        2         1\n",
       "      1         2         2           2        4         4         1        4         2\n",
       "      5         2         3           4        3         2         1        0         1\n",
       "      1         2         1           2        5         5         1        4         2\n",
       "      4         4         4           5        1         1         3        2         0\n",
       "      1         1         1           1        3         4         1        4         2\n",
       "      2         3         3           3        3         3         3        2         1\n",
       "      1         5         2           2        2         5         1        3         1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stress = DataTable(['sleep', 'living', 'basics', 'academic', 'study', 'career', 'social', 'extra', 'stress'])\n",
    "stress.load('student-stress.txt')\n",
    "stress.rows(range(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7a367e",
   "metadata": {},
   "source": [
    "## Step 1: Initial kNN and Naive Bayes Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db71c6bf",
   "metadata": {},
   "source": [
    "*TODO: Similar to the titanic analysis above, use stratified k-fold cross validation to evaluate knn and naive bayes for predicting student stress level (the stress attribute) using the other table attributes as categorical values. For both evaluations use 10 folds, and for knn use a k-value of 7 and majority voting. Give your resulting confusion matrices as well as accuracy, precision, recall, and f-measure values.*\n",
    "\n",
    "Note that depending on your implementations, it may take some time for Python to finish running your evaluations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual      0    1    2    total\n",
      "--------  ---  ---  ---  -------\n",
      "0         373    0    0      373\n",
      "1         126  231    1      358\n",
      "2          69    0  300      369\n",
      "total     568  231  301     1100\n",
      "\n",
      "Non-empty labels count: 3\n",
      "Average accuracy: 0.8812121212121212\n",
      "Precision: 0.8844559605696193\n",
      "Recall: 0.8194198422431151\n",
      "Macro-averaging F-score: 0.850696702757969\n"
     ]
    }
   ],
   "source": [
    "# kNN\n",
    "stress_copy = stress.copy()\n",
    "result = knn_stratified(stress_copy, 10, 'stress', majority_vote, 7, [], ['sleep', 'living', 'basics', 'academic', 'study', 'career', 'social', 'extra'])\n",
    "print(result)\n",
    "print()\n",
    "label_list = result.get_column_data('actual')\n",
    "if 'total' in label_list:\n",
    "    label_list.remove('total')\n",
    "\n",
    "# Find non-empty labels\n",
    "sum_of_actual_labels = result.get_column_data('total')\n",
    "non_empty_label = 0\n",
    "for i in sum_of_actual_labels:\n",
    "    if i != 0:\n",
    "        non_empty_label += 1\n",
    "\n",
    "if result[result.row_count() - 1]['actual'] == 'total':\n",
    "    non_empty_label -= 1\n",
    "\n",
    "print(\"Non-empty labels count:\", non_empty_label)\n",
    "\n",
    "# Accuracy\n",
    "total_accuracy = 0\n",
    "for i in label_list: \n",
    "    num = accuracy(result, i)\n",
    "    if num != -1:\n",
    "        total_accuracy += num\n",
    "acc = total_accuracy / non_empty_label\n",
    "print(\"Average accuracy:\", acc)\n",
    "\n",
    "# Precision\n",
    "total_precision = 0\n",
    "denum = non_empty_label\n",
    "for i in label_list:\n",
    "    num = precision(result, i)\n",
    "    if num != -1:\n",
    "        total_precision += num\n",
    "    else:\n",
    "        print(\"p_prediced is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "        non_empty_label -= 1\n",
    "precision_m = total_precision / denum\n",
    "print(\"Precision:\", precision_m)\n",
    "\n",
    "# Recall\n",
    "total_recall = 0\n",
    "denum = non_empty_label\n",
    "for i in label_list: # labels are 1, 2, 3\n",
    "    num = recall(result, i)\n",
    "    if num != -1:\n",
    "        total_recall += num\n",
    "    else:\n",
    "        print(\"p_actual is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "        denum -= 1\n",
    "recall_m = total_recall / non_empty_label\n",
    "print(\"Recall:\", recall_m)\n",
    "\n",
    "# F-score\n",
    "f_score = 2 * precision_m * recall_m / (precision_m + recall_m)\n",
    "print(\"Macro-averaging F-score:\", f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual      0    1    2    total\n",
      "--------  ---  ---  ---  -------\n",
      "0         322   16   35      373\n",
      "1          20  314   24      358\n",
      "2          22   15  332      369\n",
      "total     364  345  391     1100\n",
      "\n",
      "Non-empty labels count: 3\n",
      "Average accuracy: 0.9199999999999999\n",
      "Precision: 0.8812883904955516\n",
      "Recall: 0.8800315822789683\n",
      "Macro-averaging F-score: 0.8806595379829616\n"
     ]
    }
   ],
   "source": [
    "# kNN\n",
    "stress_copy = stress.copy()\n",
    "result = naive_bayes_stratified(stress_copy, 10, 'stress', [], ['sleep', 'living', 'basics', 'academic', 'study', 'career', 'social', 'extra'])\n",
    "print(result)\n",
    "print()\n",
    "label_list = result.get_column_data('actual')\n",
    "if 'total' in label_list:\n",
    "    label_list.remove('total')\n",
    "\n",
    "# Find non-empty labels\n",
    "sum_of_actual_labels = result.get_column_data('total')\n",
    "non_empty_label = 0\n",
    "for i in sum_of_actual_labels:\n",
    "    if i != 0:\n",
    "        non_empty_label += 1\n",
    "\n",
    "if result[result.row_count() - 1]['actual'] == 'total':\n",
    "    non_empty_label -= 1\n",
    "\n",
    "print(\"Non-empty labels count:\", non_empty_label)\n",
    "\n",
    "# Accuracy\n",
    "total_accuracy = 0\n",
    "for i in label_list: \n",
    "    num = accuracy(result, i)\n",
    "    if num != -1:\n",
    "        total_accuracy += num\n",
    "acc = total_accuracy / non_empty_label\n",
    "print(\"Average accuracy:\", acc)\n",
    "\n",
    "# Precision\n",
    "total_precision = 0\n",
    "denum = non_empty_label\n",
    "for i in label_list:\n",
    "    num = precision(result, i)\n",
    "    if num != -1:\n",
    "        total_precision += num\n",
    "    else:\n",
    "        print(\"p_prediced is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "        non_empty_label -= 1\n",
    "precision_m = total_precision / denum\n",
    "print(\"Precision:\", precision_m)\n",
    "\n",
    "# Recall\n",
    "total_recall = 0\n",
    "denum = non_empty_label\n",
    "for i in label_list: # labels are 1, 2, 3\n",
    "    num = recall(result, i)\n",
    "    if num != -1:\n",
    "        total_recall += num\n",
    "    else:\n",
    "        print(\"p_actual is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "        denum -= 1\n",
    "recall_m = total_recall / non_empty_label\n",
    "print(\"Recall:\", recall_m)\n",
    "\n",
    "# F-score\n",
    "f_score = 2 * precision_m * recall_m / (precision_m + recall_m)\n",
    "print(\"Macro-averaging F-score:\", f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Observations\n",
    "\n",
    "* The results for both kNN and Naive Bayes are pretty impressive. They both yield a very high accuracy, precision, recall, and f-measure, with Naive Bayes has a bit higher statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7247cb1",
   "metadata": {},
   "source": [
    "## Step 2: Experimentation with Student Stress Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f127d0d2",
   "metadata": {},
   "source": [
    "*TODO: Rerun your evaluations in Step 1 by experimenting with different knn k-values and folds (similar to the Auto MPG steps). Display, comment on, and analyze the results of your experiments.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Run it again but at a lower number of attributes\n",
    "\n",
    "* Not factoring social and extra attributes, meaning work and professional life only\n",
    "\n",
    "kNN result\n",
    "* Increase k to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual       0    1    2    total\n",
      "--------  ----  ---  ---  -------\n",
      "0          373    0    0      373\n",
      "1          358    0    0      358\n",
      "2          369    0    0      369\n",
      "total     1100    0    0     1100\n",
      "\n",
      "Non-empty labels count: 3\n",
      "Average accuracy: 0.5593939393939394\n",
      "p_prediced is 0 for label 1 . Subtracted 1 from non_empty_label.\n",
      "p_prediced is 0 for label 2 . Subtracted 1 from non_empty_label.\n",
      "Precision: 0.11303030303030304\n",
      "Recall: 1.0\n",
      "Macro-averaging F-score: 0.20310372992104547\n"
     ]
    }
   ],
   "source": [
    "# kNN\n",
    "stress_copy = stress.copy()\n",
    "result = knn_stratified(stress_copy, 10, 'stress', majority_vote, 10, [], ['sleep', 'living', 'basics', 'academic', 'study', 'career'])\n",
    "print(result)\n",
    "print()\n",
    "label_list = result.get_column_data('actual')\n",
    "if 'total' in label_list:\n",
    "    label_list.remove('total')\n",
    "\n",
    "# Find non-empty labels\n",
    "sum_of_actual_labels = result.get_column_data('total')\n",
    "non_empty_label = 0\n",
    "for i in sum_of_actual_labels:\n",
    "    if i != 0:\n",
    "        non_empty_label += 1\n",
    "\n",
    "if result[result.row_count() - 1]['actual'] == 'total':\n",
    "    non_empty_label -= 1\n",
    "\n",
    "print(\"Non-empty labels count:\", non_empty_label)\n",
    "\n",
    "# Accuracy\n",
    "total_accuracy = 0\n",
    "for i in label_list: \n",
    "    num = accuracy(result, i)\n",
    "    if num != -1:\n",
    "        total_accuracy += num\n",
    "acc = total_accuracy / non_empty_label\n",
    "print(\"Average accuracy:\", acc)\n",
    "\n",
    "# Precision\n",
    "total_precision = 0\n",
    "denum = non_empty_label\n",
    "for i in label_list:\n",
    "    num = precision(result, i)\n",
    "    if num != -1:\n",
    "        total_precision += num\n",
    "    else:\n",
    "        print(\"p_prediced is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "        non_empty_label -= 1\n",
    "precision_m = total_precision / denum\n",
    "print(\"Precision:\", precision_m)\n",
    "\n",
    "# Recall\n",
    "total_recall = 0\n",
    "denum = non_empty_label\n",
    "for i in label_list: # labels are 1, 2, 3\n",
    "    num = recall(result, i)\n",
    "    if num != -1:\n",
    "        total_recall += num\n",
    "    else:\n",
    "        print(\"p_actual is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "        denum -= 1\n",
    "recall_m = total_recall / non_empty_label\n",
    "print(\"Recall:\", recall_m)\n",
    "\n",
    "# F-score\n",
    "f_score = 2 * precision_m * recall_m / (precision_m + recall_m)\n",
    "print(\"Macro-averaging F-score:\", f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual      0    1    2    total\n",
      "--------  ---  ---  ---  -------\n",
      "0         327   22   24      373\n",
      "1          22  317   19      358\n",
      "2          26   20  323      369\n",
      "total     375  359  366     1100\n",
      "\n",
      "Non-empty labels count: 3\n",
      "Average accuracy: 0.9193939393939394\n",
      "Precision: 0.8791740059160489\n",
      "Recall: 0.8791630723132959\n",
      "Macro-averaging F-score: 0.879168539080679\n"
     ]
    }
   ],
   "source": [
    "# kNN\n",
    "stress_copy = stress.copy()\n",
    "result = naive_bayes_stratified(stress_copy, 10, 'stress', [], ['sleep', 'living', 'basics', 'academic', 'study', 'career'])\n",
    "print(result)\n",
    "print()\n",
    "label_list = result.get_column_data('actual')\n",
    "if 'total' in label_list:\n",
    "    label_list.remove('total')\n",
    "\n",
    "# Find non-empty labels\n",
    "sum_of_actual_labels = result.get_column_data('total')\n",
    "non_empty_label = 0\n",
    "for i in sum_of_actual_labels:\n",
    "    if i != 0:\n",
    "        non_empty_label += 1\n",
    "\n",
    "if result[result.row_count() - 1]['actual'] == 'total':\n",
    "    non_empty_label -= 1\n",
    "\n",
    "print(\"Non-empty labels count:\", non_empty_label)\n",
    "\n",
    "# Accuracy\n",
    "total_accuracy = 0\n",
    "for i in label_list: \n",
    "    num = accuracy(result, i)\n",
    "    if num != -1:\n",
    "        total_accuracy += num\n",
    "acc = total_accuracy / non_empty_label\n",
    "print(\"Average accuracy:\", acc)\n",
    "\n",
    "# Precision\n",
    "total_precision = 0\n",
    "denum = non_empty_label\n",
    "for i in label_list:\n",
    "    num = precision(result, i)\n",
    "    if num != -1:\n",
    "        total_precision += num\n",
    "    else:\n",
    "        print(\"p_prediced is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "        non_empty_label -= 1\n",
    "precision_m = total_precision / denum\n",
    "print(\"Precision:\", precision_m)\n",
    "\n",
    "# Recall\n",
    "total_recall = 0\n",
    "denum = non_empty_label\n",
    "for i in label_list: # labels are 1, 2, 3\n",
    "    num = recall(result, i)\n",
    "    if num != -1:\n",
    "        total_recall += num\n",
    "    else:\n",
    "        print(\"p_actual is 0 for label\", i, \". Subtracted 1 from non_empty_label.\")\n",
    "        denum -= 1\n",
    "recall_m = total_recall / non_empty_label\n",
    "print(\"Recall:\", recall_m)\n",
    "\n",
    "# F-score\n",
    "f_score = 2 * precision_m * recall_m / (precision_m + recall_m)\n",
    "print(\"Macro-averaging F-score:\", f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Observations\n",
    "\n",
    "* The result returned is very interesting. If I get rid of the social and extra attributes, the result for kNN is pretty bad. This may suggest that social and extra are two attributes that are highly correlated.\n",
    "* This further affirms the fact that kNN is very sensitive to data skew.\n",
    "* Naive Bayes, on the other hand, is not affected by this change. It still produces a very high accuracy, precision, recall, and f-measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6390bb",
   "metadata": {},
   "source": [
    "# 4. Issues, Challenges, and Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf6b940",
   "metadata": {},
   "source": [
    "1. Issues and Challenges\n",
    "* I don't have any issues or challenges with this assignment.\n",
    "\n",
    "2. Obersevations\n",
    "* Overall, as shown in both Section 2 and 3, the kNN algorithm only works well if the parameter is set up correctly. This turns out to be quite tricky in some cases, especially when we have to investigate what is the best setting for the algorithm to work properly - which can be expensive sometimes.\n",
    "* Naive Bayes, on the other hand, is much more robust and works well in most cases. However, the down size is that it runs pretty slow(er) than kNN.\n",
    "* In terms of accuracy, precision, recall, and f-measure, Naive Bayes is the clear winner. It produces a much higher result than kNN most of the time. The only time when kNN has a better result is suring Section 2, when the kNN parameter is set up differntly. However, the variation between two algorithm results is not that significant in that case.\n",
    "* Comparing to the previous assignment when I choose the train and test set using holdout, I find that k-stratified cross validation is much better. It produces a more representative test set, which in turn produces a better and more reliable result.\n",
    "* For kNN, the higher the k value, the generally better result we will yield. However, it is not always the case. Sometimes, the result will be worse if we increase the k value. This is because the algorithm is very sensitive to data skew. If the data is skewed, the algorithm will not work properly. Hence, if we can put it in a loop and stop until the marginal increase is less 0, we can stop there and get the best result.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
